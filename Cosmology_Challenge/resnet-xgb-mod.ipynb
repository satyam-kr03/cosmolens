{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d3a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac88b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    @staticmethod\n",
    "    def add_noise(data, mask, ng, pixel_size=2.):\n",
    "        return data + np.random.randn(*data.shape) * 0.4 / (2*ng*pixel_size**2)**0.5 * mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_np(data_dir, file_name):\n",
    "        return np.load(os.path.join(data_dir, file_name))\n",
    "\n",
    "    @staticmethod\n",
    "    def save_np(data_dir, file_name, data):\n",
    "        np.save(os.path.join(data_dir, file_name), data)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_json_zip(submission_dir, json_file_name, zip_file_name, data):\n",
    "        os.makedirs(submission_dir, exist_ok=True)\n",
    "        json_path = os.path.join(submission_dir, json_file_name)\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "        zip_path = os.path.join(submission_dir, zip_file_name)\n",
    "        with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "            zf.write(json_path, arcname=json_file_name)\n",
    "        os.remove(json_path)\n",
    "        return zip_path\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, data_dir, USE_PUBLIC_DATASET):\n",
    "        self.USE_PUBLIC_DATASET = USE_PUBLIC_DATASET\n",
    "        self.data_dir = data_dir\n",
    "        self.mask_file = 'WIDE12H_bin2_2arcmin_mask.npy'\n",
    "        self.viz_label_file = 'label.npy'\n",
    "        \n",
    "        if self.USE_PUBLIC_DATASET:\n",
    "            self.kappa_file = 'WIDE12H_bin2_2arcmin_kappa.npy'\n",
    "            self.label_file = self.viz_label_file\n",
    "            self.Ncosmo = 101\n",
    "            self.Nsys = 256\n",
    "            self.test_kappa_file = 'WIDE12H_bin2_2arcmin_kappa_noisy_test.npy'\n",
    "            self.Ntest = 4000\n",
    "        else:\n",
    "            self.kappa_file = 'sampled_WIDE12H_bin2_2arcmin_kappa.npy'\n",
    "            self.label_file = 'sampled_label.npy'\n",
    "            self.Ncosmo = 3\n",
    "            self.Nsys = 30\n",
    "            self.test_kappa_file = 'sampled_WIDE12H_bin2_2arcmin_kappa_noisy_test.npy'\n",
    "            self.Ntest = 3\n",
    "        \n",
    "        self.shape = [1424, 176]\n",
    "        self.pixelsize_arcmin = 2\n",
    "        self.pixelsize_radian = self.pixelsize_arcmin / 60 / 180 * np.pi\n",
    "        self.ng = 30\n",
    "\n",
    "    def load_train_data(self):\n",
    "        self.mask = Utility.load_np(data_dir=self.data_dir, file_name=self.mask_file)\n",
    "        self.kappa = np.zeros((self.Ncosmo, self.Nsys, *self.shape), dtype=np.float16)\n",
    "        self.kappa[:,:,self.mask] = Utility.load_np(data_dir=self.data_dir, file_name=self.kappa_file)\n",
    "        self.label = Utility.load_np(data_dir=self.data_dir, file_name=self.label_file)\n",
    "        self.viz_label = Utility.load_np(data_dir=self.data_dir, file_name=self.viz_label_file)\n",
    "\n",
    "    def load_test_data(self):\n",
    "        self.kappa_test = np.zeros((self.Ntest, *self.shape), dtype=np.float16)\n",
    "        self.kappa_test[:,self.mask] = Utility.load_np(data_dir=self.data_dir, file_name=self.test_kappa_file)\n",
    "\n",
    "\n",
    "class Score:\n",
    "    @staticmethod\n",
    "    def _score_phase1(true_cosmo, infer_cosmo, errorbar):\n",
    "        sq_error = (true_cosmo - infer_cosmo)**2\n",
    "        scale_factor = 1000\n",
    "        score = - np.sum(sq_error / errorbar**2 + np.log(errorbar**2) + scale_factor * sq_error, 1)\n",
    "        score = np.mean(score)\n",
    "        return score if score >= -10**6 else -10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score:\n",
    "    @staticmethod\n",
    "    def _score_phase1(true_cosmo, infer_cosmo, errorbar):\n",
    "        sq_error = (true_cosmo - infer_cosmo)**2\n",
    "        scale_factor = 1000\n",
    "        score = - np.sum(sq_error / errorbar**2 + np.log(errorbar**2) + scale_factor * sq_error, 1)\n",
    "        score = np.mean(score)\n",
    "        return score if score >= -10**6 else -10**6\n",
    "    \n",
    "    @staticmethod\n",
    "    def competition_loss(predictions, targets, errorbar_proxy=None, scale_factor=1000.0):\n",
    "        \"\"\"\n",
    "        Competition-aware loss function for training.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions (batch_size, num_targets)\n",
    "            targets: Ground truth targets (batch_size, num_targets)\n",
    "            errorbar_proxy: Proxy for error bars. If None, uses prediction std or fixed value\n",
    "            scale_factor: Weight for squared error term (default 1000 as in scoring)\n",
    "        \n",
    "        Returns:\n",
    "            loss: Competition-aware loss (to be minimized)\n",
    "        \"\"\"\n",
    "        sq_error = (predictions - targets)**2\n",
    "        \n",
    "        # Use proxy for error bars if not provided\n",
    "        if errorbar_proxy is None:\n",
    "            # Use a fixed reasonable error bar proxy based on target scale\n",
    "            errorbar_proxy = torch.ones_like(predictions) * 0.05  # ~5% relative error\n",
    "        \n",
    "        # Competition score components (negated since we want to minimize)\n",
    "        # score = - (sq_error/errorbar^2 + log(errorbar^2) + scale_factor * sq_error)\n",
    "        # So loss = sq_error/errorbar^2 + log(errorbar^2) + scale_factor * sq_error\n",
    "        \n",
    "        loss = (sq_error / (errorbar_proxy**2)).mean() + \\\n",
    "               (torch.log(errorbar_proxy**2)).mean() + \\\n",
    "               scale_factor * sq_error.mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def calibrate_error_bars_on_validation(val_predictions, val_true, val_error_bars, \n",
    "                                          n_trials=50, errorbar_range=(0.01, 0.5)):\n",
    "        \"\"\"\n",
    "        Calibrate error bars by optimizing competition score on validation set.\n",
    "        \n",
    "        Args:\n",
    "            val_predictions: Predictions on validation set\n",
    "            val_true: True values on validation set  \n",
    "            val_error_bars: Current error bars from MCMC\n",
    "            n_trials: Number of calibration trials\n",
    "            errorbar_range: Range of scaling factors to try\n",
    "        \n",
    "        Returns:\n",
    "            calibrated_error_bars: Optimized error bars\n",
    "            best_scale: Best scaling factor\n",
    "        \"\"\"\n",
    "        print(\"Calibrating error bars for competition score...\")\n",
    "        \n",
    "        best_score = float('-inf')\n",
    "        best_scale = 1.0\n",
    "        \n",
    "        for scale in np.linspace(errorbar_range[0], errorbar_range[1], n_trials):\n",
    "            scaled_errorbars = val_error_bars * scale\n",
    "            score = Score._score_phase1(val_true, val_predictions, scaled_errorbars)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_scale = scale\n",
    "        \n",
    "        calibrated_error_bars = val_error_bars * best_scale\n",
    "        \n",
    "        print(f\"Best error bar scale: {best_scale:.3f} (Score: {best_score:.2f})\")\n",
    "        \n",
    "        return calibrated_error_bars, best_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e98442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiScaleResNet(nn.Module):\n",
    "    def __init__(self, height, width, feature_dim=256, dropout=0.3):\n",
    "        super(MultiScaleResNet, self).__init__()\n",
    "        \n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool1 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.avgpool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.avgpool3 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.avgpool4 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 + 128 + 256 + 512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, feature_dim),\n",
    "            nn.BatchNorm1d(feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        \n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        \n",
    "        f1 = self.avgpool1(x1).view(x1.size(0), -1)\n",
    "        f2 = self.avgpool2(x2).view(x2.size(0), -1)\n",
    "        f3 = self.avgpool3(x3).view(x3.size(0), -1)\n",
    "        f4 = self.avgpool4(x4).view(x4.size(0), -1)\n",
    "        \n",
    "        multi_scale = torch.cat([f1, f2, f3, f4], dim=1)\n",
    "        features = self.fc(multi_scale)\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "class MultiScaleResNetWithHead(nn.Module):\n",
    "    def __init__(self, height, width, num_targets, feature_dim=256, dropout=0.3):\n",
    "        super(MultiScaleResNetWithHead, self).__init__()\n",
    "        self.feature_extractor = MultiScaleResNet(height, width, feature_dim, dropout)\n",
    "        self.prediction_head = nn.Linear(feature_dim, num_targets)\n",
    "        \n",
    "    def forward(self, x, return_features=False):\n",
    "        features = self.feature_extractor(x)\n",
    "        if return_features:\n",
    "            return features\n",
    "        predictions = self.prediction_head(features)\n",
    "        return predictions\n",
    "    \n",
    "    def get_feature_extractor(self):\n",
    "        return self.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80303d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistical_features(images):\n",
    "    \"\"\"\n",
    "    Compute enhanced statistical and physics-based features from images.\n",
    "    Args:\n",
    "        images: numpy array of shape (N, H, W)\n",
    "    Returns:\n",
    "        features: numpy array of shape (N, n_features)\n",
    "    \"\"\"\n",
    "    N = images.shape[0]\n",
    "    features_list = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        img = images[i]\n",
    "        img_flat = img.flatten()\n",
    "        img_flat = img_flat[img_flat != 0]  # Remove masked values\n",
    "        \n",
    "        if len(img_flat) == 0:\n",
    "            features_list.append(np.zeros(50))  # Increased feature count\n",
    "            continue\n",
    "        \n",
    "        # Basic statistical features (8 features)\n",
    "        basic_features = [\n",
    "            np.mean(img_flat),\n",
    "            np.std(img_flat),\n",
    "            skew(img_flat),\n",
    "            kurtosis(img_flat),\n",
    "            np.percentile(img_flat, 25),\n",
    "            np.percentile(img_flat, 75),\n",
    "            np.min(img_flat),\n",
    "            np.max(img_flat)\n",
    "        ]\n",
    "        \n",
    "        # Power spectrum features (12 features)\n",
    "        power_features = compute_power_spectrum_features(img)\n",
    "        \n",
    "        # Peak statistics (6 features)\n",
    "        peak_features = compute_peak_statistics(img)\n",
    "        \n",
    "        # Minkowski functionals (4 features)\n",
    "        minkowski_features = compute_minkowski_functionals(img)\n",
    "        \n",
    "        # Correlation function features (8 features)\n",
    "        correlation_features = compute_correlation_features(img)\n",
    "        \n",
    "        # Wavelet features (12 features)\n",
    "        wavelet_features = compute_wavelet_features(img)\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = (basic_features + power_features + peak_features + \n",
    "                       minkowski_features + correlation_features + wavelet_features)\n",
    "        \n",
    "        features_list.append(all_features)\n",
    "    \n",
    "    return np.array(features_list, dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_power_spectrum_features(img, n_bins=6):\n",
    "    \"\"\"\n",
    "    Compute power spectrum statistics from 2D image.\n",
    "    Returns: [total_power, slope, power_at_k1, power_at_k2, ..., power_at_k6]\n",
    "    \"\"\"\n",
    "    # Compute 2D power spectrum\n",
    "    fft_img = np.fft.fft2(img)\n",
    "    power_spectrum = np.abs(fft_img)**2\n",
    "    \n",
    "    # Get radial power spectrum\n",
    "    h, w = img.shape\n",
    "    center = (h//2, w//2)\n",
    "    \n",
    "    # Create radial bins\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    r = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n",
    "    r_max = min(center)\n",
    "    \n",
    "    # Bin the power spectrum\n",
    "    k_bins = np.linspace(0, r_max, n_bins + 1)\n",
    "    power_radial = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = (r >= k_bins[i]) & (r < k_bins[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            power_radial.append(np.mean(power_spectrum[mask]))\n",
    "        else:\n",
    "            power_radial.append(0.0)\n",
    "    \n",
    "    # Compute slope of power spectrum (log-log)\n",
    "    k_values = np.arange(1, len(power_radial))\n",
    "    log_k = np.log(k_values)\n",
    "    log_p = np.log(np.array(power_radial[1:]) + 1e-10)\n",
    "    \n",
    "    if len(k_values) > 1:\n",
    "        slope = np.polyfit(log_k, log_p, 1)[0]\n",
    "    else:\n",
    "        slope = 0.0\n",
    "    \n",
    "    total_power = np.sum(power_spectrum)\n",
    "    \n",
    "    return [total_power, slope] + power_radial\n",
    "\n",
    "\n",
    "def compute_peak_statistics(img, threshold_percentile=95):\n",
    "    \"\"\"\n",
    "    Compute peak statistics from the image.\n",
    "    Returns: [n_peaks, mean_peak_height, max_peak_height, peak_clustering, peak_density, peak_dispersion]\n",
    "    \"\"\"\n",
    "    # Find peaks above threshold\n",
    "    threshold = np.percentile(img, threshold_percentile)\n",
    "    peaks = img > threshold\n",
    "    \n",
    "    if np.sum(peaks) == 0:\n",
    "        return [0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    # Label connected components (peaks)\n",
    "    from scipy import ndimage\n",
    "    labeled_peaks, n_peaks = ndimage.label(peaks)\n",
    "    \n",
    "    # Peak heights\n",
    "    peak_heights = img[peaks]\n",
    "    mean_peak_height = np.mean(peak_heights)\n",
    "    max_peak_height = np.max(peak_heights)\n",
    "    \n",
    "    # Peak clustering (average distance between peaks)\n",
    "    peak_coords = np.array(np.where(peaks)).T\n",
    "    if len(peak_coords) > 1:\n",
    "        # Compute pairwise distances and take mean minimum distance\n",
    "        from scipy.spatial.distance import pdist\n",
    "        distances = pdist(peak_coords)\n",
    "        peak_clustering = np.mean(np.sort(distances)[:len(peak_coords)//2])\n",
    "    else:\n",
    "        peak_clustering = 0.0\n",
    "    \n",
    "    # Peak density (peaks per unit area)\n",
    "    peak_density = n_peaks / (img.shape[0] * img.shape[1])\n",
    "    \n",
    "    # Peak dispersion (variance of peak heights)\n",
    "    peak_dispersion = np.var(peak_heights)\n",
    "    \n",
    "    return [n_peaks, mean_peak_height, max_peak_height, peak_clustering, peak_density, peak_dispersion]\n",
    "\n",
    "\n",
    "def compute_minkowski_functionals(img, n_thresholds=4):\n",
    "    \"\"\"\n",
    "    Compute Minkowski functionals (topology statistics).\n",
    "    Returns: [area, perimeter, euler_characteristic] for different thresholds\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(np.min(img), np.max(img), n_thresholds + 2)[1:-1]\n",
    "    \n",
    "    functionals = []\n",
    "    for thresh in thresholds:\n",
    "        binary = img > thresh\n",
    "        \n",
    "        # Area (volume)\n",
    "        area = np.sum(binary)\n",
    "        \n",
    "        # Perimeter (surface area) - approximate using gradient\n",
    "        from scipy import ndimage\n",
    "        grad_x = ndimage.sobel(binary.astype(float), axis=0)\n",
    "        grad_y = ndimage.sobel(binary.astype(float), axis=1)\n",
    "        perimeter = np.sum(np.sqrt(grad_x**2 + grad_y**2) > 0)\n",
    "        \n",
    "        # Euler characteristic (connected components - holes)\n",
    "        labeled, n_components = ndimage.label(binary)\n",
    "        # Simple approximation: components minus holes (not exact for 2D)\n",
    "        euler_char = n_components\n",
    "        \n",
    "        functionals.extend([area, perimeter, euler_char])\n",
    "    \n",
    "    # Return average functionals across thresholds\n",
    "    functionals = np.array(functionals).reshape(n_thresholds, 3)\n",
    "    return np.mean(functionals, axis=0).tolist()\n",
    "\n",
    "\n",
    "def compute_correlation_features(img, scales=[1, 2, 4, 8]):\n",
    "    \"\"\"\n",
    "    Compute correlation function at different scales.\n",
    "    Returns: correlation values at specified scales\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Simple auto-correlation approximation\n",
    "        corr = np.corrcoef(img[:-scale, :-scale].flatten(), \n",
    "                          img[scale:, scale:].flatten())[0, 1]\n",
    "        correlations.append(corr if not np.isnan(corr) else 0.0)\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "\n",
    "def compute_wavelet_features(img, n_levels=3):\n",
    "    \"\"\"\n",
    "    Compute wavelet decomposition features.\n",
    "    Returns: statistics from wavelet coefficients at different scales\n",
    "    \"\"\"\n",
    "    # Simple Haar wavelet-like decomposition (approximation)\n",
    "    features = []\n",
    "    \n",
    "    current = img.astype(float)\n",
    "    for level in range(n_levels):\n",
    "        if current.shape[0] < 4 or current.shape[1] < 4:\n",
    "            break\n",
    "            \n",
    "        # Approximate wavelet decomposition\n",
    "        h, w = current.shape\n",
    "        h2, w2 = h//2, w//2\n",
    "        \n",
    "        # Low-low (approximation)\n",
    "        approx = current[:h2*2:2, :w2*2:2]\n",
    "        \n",
    "        # High-low (vertical details)\n",
    "        vert = current[1:h2*2:2, :w2*2:2] - current[:h2*2:2, :w2*2:2]\n",
    "        \n",
    "        # Low-high (horizontal details)  \n",
    "        horiz = current[:h2*2:2, 1:w2*2:2] - current[:h2*2:2, :w2*2:2]\n",
    "        \n",
    "        # High-high (diagonal details)\n",
    "        diag = current[1:h2*2:2, 1:w2*2:2] - current[:h2*2:2, :w2*2:2]\n",
    "        \n",
    "        # Extract statistics from detail coefficients\n",
    "        for detail in [vert, horiz, diag]:\n",
    "            if detail.size > 0:\n",
    "                features.extend([\n",
    "                    np.mean(np.abs(detail)),\n",
    "                    np.std(detail),\n",
    "                    np.max(np.abs(detail)),\n",
    "                    skew(detail.flatten()) if len(detail.flatten()) > 2 else 0\n",
    "                ])\n",
    "        \n",
    "        current = approx\n",
    "    \n",
    "    # Pad or truncate to fixed size\n",
    "    target_size = 12\n",
    "    if len(features) < target_size:\n",
    "        features.extend([0] * (target_size - len(features)))\n",
    "    else:\n",
    "        features = features[:target_size]\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97004050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridFeatureExtractor:\n",
    "    def __init__(self, feature_extractor, device, use_pca=True, pca_components=128):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = device\n",
    "        self.use_pca = use_pca\n",
    "        self.pca_components = pca_components\n",
    "        self.pca = None\n",
    "        self.stat_scaler = StandardScaler()\n",
    "        \n",
    "    def extract_features(self, dataloader, fit_transforms=False):\n",
    "        self.feature_extractor.eval()\n",
    "        cnn_features_list = []\n",
    "        images_for_stats = []\n",
    "        labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Extracting CNN features\"):\n",
    "                if len(batch) == 2:\n",
    "                    X, y = batch\n",
    "                    labels_list.append(y.numpy())\n",
    "                else:\n",
    "                    X = batch\n",
    "                \n",
    "                X_device = X.to(self.device)\n",
    "                # Extract features from the feature extractor\n",
    "                features = self.feature_extractor(X_device)\n",
    "                cnn_features_list.append(features.cpu().numpy())\n",
    "                \n",
    "                # Store denormalized images for statistical features\n",
    "                images_for_stats.append(X.numpy().squeeze(1))\n",
    "        \n",
    "        cnn_features = np.concatenate(cnn_features_list, axis=0)\n",
    "        images = np.concatenate(images_for_stats, axis=0)\n",
    "        \n",
    "        print(\"Computing statistical features...\")\n",
    "        stat_features = compute_statistical_features(images)\n",
    "        \n",
    "        if fit_transforms:\n",
    "            stat_features = self.stat_scaler.fit_transform(stat_features)\n",
    "        else:\n",
    "            stat_features = self.stat_scaler.transform(stat_features)\n",
    "        \n",
    "        combined_features = np.concatenate([cnn_features, stat_features], axis=1)\n",
    "        \n",
    "        if self.use_pca:\n",
    "            if fit_transforms:\n",
    "                self.pca = PCA(n_components=min(self.pca_components, combined_features.shape[1]))\n",
    "                combined_features = self.pca.fit_transform(combined_features)\n",
    "                print(f\"PCA explained variance: {self.pca.explained_variance_ratio_.sum():.4f}\")\n",
    "            else:\n",
    "                combined_features = self.pca.transform(combined_features)\n",
    "        \n",
    "        if labels_list:\n",
    "            labels = np.concatenate(labels_list, axis=0)\n",
    "            return combined_features, labels\n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedEnsemble:\n",
    "    def __init__(self, cnn_model, device, xgb_params=None, pca_components=128):\n",
    "        self.cnn_model = cnn_model\n",
    "        self.device = device\n",
    "        self.feature_extractor = HybridFeatureExtractor(cnn_model.get_feature_extractor(), device, use_pca=True, \n",
    "                                                        pca_components=pca_components)\n",
    "        \n",
    "        if xgb_params is None:\n",
    "            xgb_params = {\n",
    "                'n_estimators': 300,\n",
    "                'max_depth': 10,\n",
    "                'learning_rate': 0.05,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'reg_alpha': 0.1,\n",
    "                'reg_lambda': 1.0,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'tree_method': 'hist'\n",
    "            }\n",
    "        self.xgb_params = xgb_params\n",
    "        self.xgb_models = []\n",
    "        self.meta_learner = None\n",
    "        \n",
    "    def fit(self, train_loader, num_targets=2, use_competition_loss=False):\n",
    "        print(\"Extracting training features...\")\n",
    "        X_train, y_train = self.feature_extractor.extract_features(train_loader, fit_transforms=True)\n",
    "        \n",
    "        print(f\"Training XGBoost models on {X_train.shape[0]} samples with {X_train.shape[1]} features...\")\n",
    "        self.xgb_models = []\n",
    "        for i in range(num_targets):\n",
    "            print(f\"Training XGBoost for target {i+1}/{num_targets}...\")\n",
    "            model = xgb.XGBRegressor(**self.xgb_params)\n",
    "            model.fit(X_train, y_train[:, i], verbose=False)\n",
    "            self.xgb_models.append(model)\n",
    "        \n",
    "        print(\"Training meta-learner (stacking)...\")\n",
    "        self.cnn_model.eval()\n",
    "        cnn_preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in train_loader:\n",
    "                X, _ = batch\n",
    "                X = X.to(self.device)\n",
    "                preds = self.cnn_model.prediction_head(self.cnn_model.feature_extractor(X))\n",
    "                cnn_preds_list.append(preds.cpu().numpy())\n",
    "        cnn_preds = np.concatenate(cnn_preds_list, axis=0)\n",
    "        \n",
    "        xgb_preds = np.column_stack([model.predict(X_train) for model in self.xgb_models])\n",
    "        \n",
    "        stacked_features = np.concatenate([cnn_preds, xgb_preds], axis=1)\n",
    "        \n",
    "        self.meta_learner = nn.Sequential(\n",
    "            nn.Linear(num_targets * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, num_targets)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.meta_learner.parameters(), lr=0.001)\n",
    "        \n",
    "        # Choose loss function\n",
    "        if use_competition_loss:\n",
    "            loss_fn = lambda pred, target: Score.competition_loss(pred, target, scale_factor=1000.0)\n",
    "            print(\"Using competition loss for meta-learner training\")\n",
    "        else:\n",
    "            loss_fn = nn.MSELoss()\n",
    "            print(\"Using MSE loss for meta-learner training\")\n",
    "        \n",
    "        X_meta = torch.FloatTensor(stacked_features).to(self.device)\n",
    "        y_meta = torch.FloatTensor(y_train).to(self.device)\n",
    "        \n",
    "        for epoch in range(100000):\n",
    "            self.meta_learner.train()\n",
    "            optimizer.zero_grad()\n",
    "            preds = self.meta_learner(X_meta)\n",
    "            loss = loss_fn(preds, y_meta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Meta-learner epoch {epoch+1}/100, Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        print(\"Ensemble training complete!\")\n",
    "        \n",
    "    def predict(self, test_loader):\n",
    "        print(\"Extracting test features...\")\n",
    "        features = self.feature_extractor.extract_features(test_loader, fit_transforms=False)\n",
    "        if isinstance(features, tuple):\n",
    "            X_test = features[0]\n",
    "        else:\n",
    "            X_test = features\n",
    "        \n",
    "        print(\"Making XGBoost predictions...\")\n",
    "        xgb_preds = np.column_stack([model.predict(X_test) for model in self.xgb_models])\n",
    "        \n",
    "        print(\"Making CNN predictions...\")\n",
    "        self.cnn_model.eval()\n",
    "        cnn_preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                if isinstance(batch, list):\n",
    "                    X = batch[0]\n",
    "                else:\n",
    "                    X = batch\n",
    "                X = X.to(self.device)\n",
    "                preds = self.cnn_model.prediction_head(self.cnn_model.feature_extractor(X))\n",
    "                cnn_preds_list.append(preds.cpu().numpy())\n",
    "        cnn_preds = np.concatenate(cnn_preds_list, axis=0)\n",
    "        \n",
    "        print(\"Combining predictions with meta-learner...\")\n",
    "        stacked_features = np.concatenate([cnn_preds, xgb_preds], axis=1)\n",
    "        X_meta = torch.FloatTensor(stacked_features).to(self.device)\n",
    "        \n",
    "        self.meta_learner.eval()\n",
    "        with torch.no_grad():\n",
    "            final_preds = self.meta_learner(X_meta).cpu().numpy()\n",
    "        \n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f962b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for X, y in pbar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X, return_features=False)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Validating\")\n",
    "    with torch.no_grad():\n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X, return_features=False)\n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383426d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedCosmologyDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, transform=None, augment=False):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx].astype(np.float32)\n",
    "        \n",
    "        if self.augment:\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = np.fliplr(image).copy()\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = np.flipud(image).copy()\n",
    "            k = np.random.randint(0, 2)*2 # only 0 or 180 degrees to preserve shape\n",
    "            if k > 0:\n",
    "                image = np.rot90(image, k).copy()\n",
    "            if np.random.rand() > 0.7:\n",
    "                noise = np.random.randn(*image.shape) * 0.01\n",
    "                image = image + noise\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            image = image.float()  # Ensure float32 type\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx].astype(np.float32)\n",
    "            label = torch.from_numpy(label)\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69debfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "USE_PUBLIC_DATASET = True\n",
    "PUBLIC_DATA_DIR = '../data'\n",
    "DATA_DIR = PUBLIC_DATA_DIR if USE_PUBLIC_DATASET else os.path.join(root_dir, 'input_data/')\n",
    "\n",
    "class Config:\n",
    "    NUM_TARGETS = 2\n",
    "    FEATURE_DIM = 256\n",
    "    DROPOUT = 0.35\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 5e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    PCA_COMPONENTS = 128\n",
    "    \n",
    "    XGB_PARAMS = {\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1.0,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    CNN_MODEL_PATH = os.path.join(root_dir, \"improved_cnn_feature_extractor.pth\")\n",
    "    ENSEMBLE_PATH = os.path.join(root_dir, \"improved_ensemble.pkl\")\n",
    "\n",
    "config = Config()\n",
    "print(f\"Device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = Data(data_dir=DATA_DIR, USE_PUBLIC_DATASET=USE_PUBLIC_DATASET)\n",
    "data_obj.load_train_data()\n",
    "data_obj.load_test_data()\n",
    "\n",
    "print(f\"Train shape: {data_obj.kappa.shape}, Test shape: {data_obj.kappa_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c643917",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_kappa_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_train.npy\")\n",
    "label_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_train.npy\")\n",
    "noisy_kappa_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_val.npy\")\n",
    "label_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_val.npy\")\n",
    "\n",
    "Ntrain = label_train.shape[0] * label_train.shape[1]\n",
    "Nval = label_val.shape[0] * label_val.shape[1]\n",
    "\n",
    "X_train = noisy_kappa_train.reshape(Ntrain, *data_obj.shape)\n",
    "X_val = noisy_kappa_val.reshape(Nval, *data_obj.shape)\n",
    "y_train = label_train.reshape(Ntrain, 5)[:, :2]\n",
    "y_val = label_val.reshape(Nval, 5)[:, :2]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6642e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X_train, dtype=np.float32)\n",
    "stds = np.std(X_train, dtype=np.float32)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[means], std=[stds]),\n",
    "])\n",
    "\n",
    "label_scaler = StandardScaler()\n",
    "y_train_scaled = label_scaler.fit_transform(y_train)\n",
    "y_val_scaled = label_scaler.transform(y_val)\n",
    "\n",
    "train_dataset = AugmentedCosmologyDataset(X_train, y_train_scaled, transform, augment=True)\n",
    "val_dataset = AugmentedCosmologyDataset(X_val, y_val_scaled, transform, augment=False)\n",
    "test_dataset = AugmentedCosmologyDataset(data_obj.kappa_test, transform=transform, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Loaders ready: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d56fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.IMG_HEIGHT = data_obj.shape[0]\n",
    "config.IMG_WIDTH = data_obj.shape[1]\n",
    "\n",
    "cnn_model = MultiScaleResNetWithHead(\n",
    "    height=config.IMG_HEIGHT,\n",
    "    width=config.IMG_WIDTH,\n",
    "    num_targets=config.NUM_TARGETS,\n",
    "    feature_dim=config.FEATURE_DIM,\n",
    "    dropout=config.DROPOUT\n",
    ").to(config.DEVICE)\n",
    "\n",
    "print(f\"Multi-scale ResNet initialized with {sum(p.numel() for p in cnn_model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_OPTUNA = False\n",
    "N_TRIALS = 5\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    print(\"Starting Optuna hyperparameter optimization...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        feature_dim = trial.suggest_categorical('feature_dim', [128, 256, 512])\n",
    "        dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "        \n",
    "        # XGBoost params\n",
    "        xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 100, 500)\n",
    "        xgb_max_depth = trial.suggest_int('xgb_max_depth', 5, 15)\n",
    "        xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.1)\n",
    "        xgb_subsample = trial.suggest_float('xgb_subsample', 0.6, 0.9)\n",
    "        xgb_colsample = trial.suggest_float('xgb_colsample', 0.6, 0.9)\n",
    "        xgb_reg_alpha = trial.suggest_float('xgb_reg_alpha', 0.01, 1.0)\n",
    "        xgb_reg_lambda = trial.suggest_float('xgb_reg_lambda', 0.1, 2.0)\n",
    "        \n",
    "        pca_components = trial.suggest_categorical('pca_components', [64, 128, 256])\n",
    "        \n",
    "        # Create dataloaders with trial batch size\n",
    "        trial_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        trial_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "        \n",
    "        # Initialize model\n",
    "        trial_model = MultiScaleResNetWithHead(\n",
    "            height=data_obj.shape[0],\n",
    "            width=data_obj.shape[1],\n",
    "            num_targets=config.NUM_TARGETS,\n",
    "            feature_dim=feature_dim,\n",
    "            dropout=dropout\n",
    "        ).to(config.DEVICE)\n",
    "        \n",
    "        # Train CNN for fewer epochs\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.AdamW(trial_model.parameters(), \n",
    "                                      lr=learning_rate, \n",
    "                                      weight_decay=weight_decay)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=1)\n",
    "        \n",
    "        n_epochs = 5  # Reduced for faster optimization\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = train_epoch(trial_model, trial_train_loader, loss_fn, optimizer, config.DEVICE)\n",
    "            val_loss = validate_epoch(trial_model, trial_val_loader, loss_fn, config.DEVICE)\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Report intermediate value for pruning\n",
    "            trial.report(val_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        # Train ensemble with trial XGBoost params\n",
    "        trial_xgb_params = {\n",
    "            'n_estimators': xgb_n_estimators,\n",
    "            'max_depth': xgb_max_depth,\n",
    "            'learning_rate': xgb_learning_rate,\n",
    "            'subsample': xgb_subsample,\n",
    "            'colsample_bytree': xgb_colsample,\n",
    "            'reg_alpha': xgb_reg_alpha,\n",
    "            'reg_lambda': xgb_reg_lambda,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        \n",
    "        trial_ensemble = StackedEnsemble(trial_model, config.DEVICE, \n",
    "                                         xgb_params=trial_xgb_params, \n",
    "                                         pca_components=pca_components)\n",
    "        trial_ensemble.fit(trial_train_loader, num_targets=config.NUM_TARGETS)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred_trial = trial_ensemble.predict(trial_val_loader)\n",
    "        y_pred_trial = label_scaler.inverse_transform(y_pred_trial)\n",
    "        \n",
    "        # Compute MSE as objective\n",
    "        mse = mean_squared_error(y_val, y_pred_trial)\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "    # Create study with pruning\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"OPTUNA OPTIMIZATION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Best MSE: {study.best_value:.6f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Update config with best parameters\n",
    "    config.FEATURE_DIM = study.best_params['feature_dim']\n",
    "    config.DROPOUT = study.best_params['dropout']\n",
    "    config.LEARNING_RATE = study.best_params['learning_rate']\n",
    "    config.BATCH_SIZE = study.best_params['batch_size']\n",
    "    config.WEIGHT_DECAY = study.best_params['weight_decay']\n",
    "    config.PCA_COMPONENTS = study.best_params['pca_components']\n",
    "    \n",
    "    config.XGB_PARAMS = {\n",
    "        'n_estimators': study.best_params['xgb_n_estimators'],\n",
    "        'max_depth': study.best_params['xgb_max_depth'],\n",
    "        'learning_rate': study.best_params['xgb_learning_rate'],\n",
    "        'subsample': study.best_params['xgb_subsample'],\n",
    "        'colsample_bytree': study.best_params['xgb_colsample'],\n",
    "        'reg_alpha': study.best_params['xgb_reg_alpha'],\n",
    "        'reg_lambda': study.best_params['xgb_reg_lambda'],\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "    \n",
    "    # Recreate dataloaders with optimized batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(\"\\nConfig updated with best hyperparameters!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping Optuna optimization (USE_OPTUNA=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED = False\n",
    "\n",
    "if not USE_PRETRAINED:\n",
    "    print(\"Training CNN feature extractor...\")\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(cnn_model.parameters(), lr=config.LEARNING_RATE, \n",
    "                                   weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 7\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_loss = train_epoch(cnn_model, train_loader, loss_fn, optimizer, config.DEVICE)\n",
    "        val_loss = validate_epoch(cnn_model, val_loader, loss_fn, config.DEVICE)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train: {train_loss:.6f} | Val: {val_loss:.6f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(cnn_model.state_dict(), config.CNN_MODEL_PATH)\n",
    "            print(f\"  âœ“ Best model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    cnn_model.load_state_dict(torch.load(config.CNN_MODEL_PATH, weights_only=True))\n",
    "    print(\"CNN training complete!\")\n",
    "    \n",
    "else:\n",
    "    cnn_model.load_state_dict(torch.load(config.CNN_MODEL_PATH, weights_only=True))\n",
    "    print(\"Loaded pretrained CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506925fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COMPETITION_TRAINING = False\n",
    "\n",
    "if USE_COMPETITION_TRAINING:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TWO-STAGE COMPETITION-BASED TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Stage 1: MSE training for accuracy (already done above)\n",
    "    print(\"Stage 1: MSE training completed\")\n",
    "    \n",
    "    # Stage 2: Competition loss fine-tuning\n",
    "    print(\"\\nStage 2: Competition loss fine-tuning...\")\n",
    "    \n",
    "    # Create a copy of the best model for fine-tuning\n",
    "    competition_model = MultiScaleResNetWithHead(\n",
    "        height=config.IMG_HEIGHT,\n",
    "        width=config.IMG_WIDTH,\n",
    "        num_targets=config.NUM_TARGETS,\n",
    "        feature_dim=config.FEATURE_DIM,\n",
    "        dropout=config.DROPOUT\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    # Load the best MSE-trained weights\n",
    "    competition_model.load_state_dict(torch.load(config.CNN_MODEL_PATH, weights_only=True))\n",
    "    \n",
    "    # Competition loss training\n",
    "    competition_epochs = 10  # Fewer epochs for fine-tuning\n",
    "    competition_lr = config.LEARNING_RATE * 0.1  # Lower learning rate\n",
    "    \n",
    "    optimizer_competition = torch.optim.AdamW(\n",
    "        competition_model.parameters(), \n",
    "        lr=competition_lr, \n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    scheduler_competition = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer_competition, T_0=3, T_mult=1\n",
    "    )\n",
    "    \n",
    "    best_competition_val_loss = float('inf')\n",
    "    patience_counter_competition = 0\n",
    "    competition_patience = 3\n",
    "    \n",
    "    for epoch in range(competition_epochs):\n",
    "        # Training with competition loss\n",
    "        competition_model.train()\n",
    "        total_competition_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Competition Training Epoch {epoch+1}\")\n",
    "        \n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(config.DEVICE), y.to(config.DEVICE)\n",
    "            pred = competition_model(X, return_features=False)\n",
    "            \n",
    "            # Use competition loss\n",
    "            loss = Score.competition_loss(pred, y, scale_factor=1000.0)\n",
    "            \n",
    "            optimizer_competition.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(competition_model.parameters(), max_norm=1.0)\n",
    "            optimizer_competition.step()\n",
    "            \n",
    "            total_competition_loss += loss.item()\n",
    "            pbar.set_postfix({'competition_loss': f'{loss.item():.6f}'})\n",
    "        \n",
    "        train_competition_loss = total_competition_loss / len(train_loader)\n",
    "        \n",
    "        # Validation with competition loss\n",
    "        competition_model.eval()\n",
    "        total_val_competition_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(config.DEVICE), y.to(config.DEVICE)\n",
    "                pred = competition_model(X, return_features=False)\n",
    "                loss = Score.competition_loss(pred, y, scale_factor=1000.0)\n",
    "                total_val_competition_loss += loss.item()\n",
    "        \n",
    "        val_competition_loss = total_val_competition_loss / len(val_loader)\n",
    "        scheduler_competition.step()\n",
    "        \n",
    "        print(f\"Competition Epoch {epoch+1}/{competition_epochs} | Train: {train_competition_loss:.6f} | Val: {val_competition_loss:.6f}\")\n",
    "        \n",
    "        if val_competition_loss < best_competition_val_loss:\n",
    "            best_competition_val_loss = val_competition_loss\n",
    "            patience_counter_competition = 0\n",
    "            torch.save(competition_model.state_dict(), config.CNN_MODEL_PATH.replace('.pth', '_competition.pth'))\n",
    "            print(\"  âœ“ Best competition model saved!\")\n",
    "        else:\n",
    "            patience_counter_competition += 1\n",
    "            if patience_counter_competition >= competition_patience:\n",
    "                print(f\"Competition training early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best competition model\n",
    "    competition_model.load_state_dict(torch.load(config.CNN_MODEL_PATH.replace('.pth', '_competition.pth'), weights_only=True))\n",
    "    print(\"Competition-based training complete!\")\n",
    "    \n",
    "    # Update cnn_model to use competition-trained weights\n",
    "    cnn_model = competition_model\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping competition-based training (USE_COMPETITION_TRAINING=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_PRETRAINED:\n",
    "    print(\"Training stacked ensemble...\")\n",
    "    \n",
    "    feature_extractor = cnn_model.get_feature_extractor()\n",
    "    ensemble = StackedEnsemble(cnn_model, config.DEVICE, xgb_params=config.XGB_PARAMS, \n",
    "                               pca_components=config.PCA_COMPONENTS)\n",
    "    ensemble.fit(train_loader, num_targets=config.NUM_TARGETS, use_competition_loss=USE_COMPETITION_TRAINING)\n",
    "    \n",
    "    with open(config.ENSEMBLE_PATH, 'wb') as f:\n",
    "        pickle.dump(ensemble, f)\n",
    "    print(f\"Ensemble saved to {config.ENSEMBLE_PATH}\")\n",
    "    \n",
    "else:\n",
    "    with open(config.ENSEMBLE_PATH, 'rb') as f:\n",
    "        ensemble = pickle.load(f)\n",
    "    print(\"Loaded pretrained ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting on validation set...\")\n",
    "y_pred_val = ensemble.predict(val_loader)\n",
    "y_pred_val = label_scaler.inverse_transform(y_pred_val)\n",
    "\n",
    "print(f\"Predictions: {y_pred_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(y_val[:,0], y_pred_val[:,0], alpha=0.5, s=15)\n",
    "axes[0].plot(sorted(y_val[:,0]), sorted(y_val[:,0]), 'k--', linewidth=2)\n",
    "axes[0].set_xlabel('Ground Truth')\n",
    "axes[0].set_ylabel('Prediction')\n",
    "axes[0].set_title(r'$\\Omega_m$')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(y_val[:,1], y_pred_val[:,1], alpha=0.5, s=15)\n",
    "axes[1].plot(sorted(y_val[:,1]), sorted(y_val[:,1]), 'k--', linewidth=2)\n",
    "axes[1].set_xlabel('Ground Truth')\n",
    "axes[1].set_ylabel('Prediction')\n",
    "axes[1].set_title(r'$S_8$')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse_om = mean_squared_error(y_val[:,0], y_pred_val[:,0])\n",
    "mse_s8 = mean_squared_error(y_val[:,1], y_pred_val[:,1])\n",
    "r2_om = r2_score(y_val[:,0], y_pred_val[:,0])\n",
    "r2_s8 = r2_score(y_val[:,1], y_pred_val[:,1])\n",
    "\n",
    "print(f\"Î©â‚˜ - MSE: {mse_om:.6f}, RÂ²: {r2_om:.4f}\")\n",
    "print(f\"Sâ‚ˆ - MSE: {mse_s8:.6f}, RÂ²: {r2_s8:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmology = data_obj.label[:,0,:2]\n",
    "Ncosmo = data_obj.Ncosmo\n",
    "\n",
    "row_to_i = {tuple(cosmology[i]): i for i in range(Ncosmo)}\n",
    "index_lists = [[] for _ in range(Ncosmo)]\n",
    "\n",
    "for idx in range(len(y_val)):\n",
    "    row_tuple = tuple(y_val[idx])\n",
    "    i = row_to_i[row_tuple]\n",
    "    index_lists[i].append(idx)\n",
    "\n",
    "val_cosmology_idx = [np.array(lst) for lst in index_lists]\n",
    "\n",
    "d_vector = []\n",
    "n_d = 2\n",
    "\n",
    "for i in range(Ncosmo):\n",
    "    d_i = np.zeros((len(val_cosmology_idx[i]), n_d))\n",
    "    for j, idx in enumerate(val_cosmology_idx[i]):\n",
    "        d_i[j] = y_pred_val[idx]\n",
    "    d_vector.append(d_i)\n",
    "\n",
    "mean_d_vector = np.array([np.mean(d_vector[i], 0) for i in range(Ncosmo)])\n",
    "delta = [d_vector[i] - mean_d_vector[i].reshape(1, n_d) for i in range(Ncosmo)]\n",
    "cov_d_vector = np.concatenate([(delta[i].T @ delta[i] / (len(delta[i])-n_d-2))[None] \n",
    "                                for i in range(Ncosmo)], 0)\n",
    "\n",
    "mean_d_vector_interp = LinearNDInterpolator(cosmology, mean_d_vector, fill_value=np.nan)\n",
    "cov_d_vector_interp = LinearNDInterpolator(cosmology, cov_d_vector, fill_value=np.nan)\n",
    "logprior_interp = LinearNDInterpolator(cosmology, np.zeros((Ncosmo, 1)), fill_value=-np.inf)\n",
    "\n",
    "def log_prior(x):\n",
    "    return logprior_interp(x).flatten()\n",
    "\n",
    "def loglike(x, d):\n",
    "    mean = mean_d_vector_interp(x)\n",
    "    cov = cov_d_vector_interp(x)\n",
    "    delta = d - mean\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    cov_det = np.linalg.slogdet(cov)[1]\n",
    "    return -0.5 * cov_det - 0.5 * np.einsum(\"ni,nij,nj->n\", delta, inv_cov, delta)\n",
    "\n",
    "def logp_posterior(x, d):\n",
    "    logp = log_prior(x)\n",
    "    select = np.isfinite(logp)\n",
    "    if np.sum(select) > 0:\n",
    "        logp[select] = logp[select] + loglike(x[select], d[select])\n",
    "    return logp\n",
    "\n",
    "print(\"MCMC setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df16859",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nstep = 10000\n",
    "sigma = 0.06\n",
    "\n",
    "current = cosmology[np.random.choice(Ncosmo, size=Nval)]\n",
    "curr_logprob = logp_posterior(current, y_pred_val)\n",
    "\n",
    "states = []\n",
    "total_acc = np.zeros(len(current))\n",
    "\n",
    "print(\"Running MCMC on validation set...\")\n",
    "\n",
    "for i in tqdm(range(Nstep), desc=\"MCMC\"):\n",
    "    proposal = current + np.random.randn(*current.shape) * sigma\n",
    "    proposal_logprob = logp_posterior(proposal, y_pred_val)\n",
    "    \n",
    "    acc_logprob = proposal_logprob - curr_logprob\n",
    "    acc_logprob[acc_logprob > 0] = 0\n",
    "    acc_prob = np.exp(acc_logprob)\n",
    "    acc = np.random.uniform(size=len(current)) < acc_prob\n",
    "    \n",
    "    total_acc += acc_prob\n",
    "    current[acc] = proposal[acc]\n",
    "    curr_logprob[acc] = proposal_logprob[acc]\n",
    "    states.append(np.copy(current)[None])\n",
    "\n",
    "states = np.concatenate(states[int(0.2*Nstep):], 0)\n",
    "mean_val = np.mean(states, 0)\n",
    "errorbar_val = np.std(states, 0)\n",
    "\n",
    "print(f\"MCMC complete! Acceptance rate: {np.mean(total_acc/Nstep):.3f}\")\n",
    "print(f\"Mean error bars: {np.mean(errorbar_val, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].errorbar(y_val[:,0], mean_val[:,0], yerr=errorbar_val[:,0],\n",
    "                 fmt='o', capsize=2, alpha=0.5, markersize=3)\n",
    "axes[0].plot(sorted(y_val[:,0]), sorted(y_val[:,0]), 'k--', linewidth=2)\n",
    "axes[0].set_xlabel('Ground Truth')\n",
    "axes[0].set_ylabel('Prediction')\n",
    "axes[0].set_title(r'$\\Omega_m$ with MCMC')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].errorbar(y_val[:,1], mean_val[:,1], yerr=errorbar_val[:,1],\n",
    "                 fmt='o', capsize=2, alpha=0.5, markersize=3)\n",
    "axes[1].plot(sorted(y_val[:,1]), sorted(y_val[:,1]), 'k--', linewidth=2)\n",
    "axes[1].set_xlabel('Ground Truth')\n",
    "axes[1].set_ylabel('Prediction')\n",
    "axes[1].set_title(r'$S_8$ with MCMC')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "validation_score = Score._score_phase1(y_val, mean_val, errorbar_val)\n",
    "print(f\"\\nValidation Score: {validation_score:.2f}\")\n",
    "print(f\"Error bar (Î©â‚˜): {np.mean(errorbar_val[:, 0]):.6f}\")\n",
    "print(f\"Error bar (Sâ‚ˆ): {np.mean(errorbar_val[:, 1]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd154f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting on test set...\")\n",
    "y_pred_test = ensemble.predict(test_loader)\n",
    "y_pred_test = label_scaler.inverse_transform(y_pred_test)\n",
    "\n",
    "print(f\"Test predictions: {y_pred_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ab336",
   "metadata": {},
   "source": [
    "# Improved Prediction Pipeline\n",
    "\n",
    "This section implements several improvements to the prediction pipeline without retraining:\n",
    "\n",
    "1. **Test-Time Augmentation (TTA)**: Average predictions over multiple augmented versions\n",
    "2. **Optimized MCMC**: Better step sizes and multiple chains\n",
    "3. **Smoother Interpolation**: Using RBF for better uncertainty estimates\n",
    "4. **Adaptive Error Calibration**: Scale error bars based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa966aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RBFInterpolator\n",
    "\n",
    "class ImprovedPredictionPipeline:\n",
    "    def __init__(self, ensemble, device, label_scaler):\n",
    "        self.ensemble = ensemble\n",
    "        self.device = device\n",
    "        self.label_scaler = label_scaler\n",
    "        \n",
    "    def predict_with_tta(self, test_loader, n_augmentations=8):\n",
    "        \"\"\"\n",
    "        Test-Time Augmentation: Average predictions over multiple augmented versions\n",
    "        \"\"\"\n",
    "        print(f\"Predicting with TTA ({n_augmentations} augmentations)...\")\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        # Get original predictions\n",
    "        y_pred = self.ensemble.predict(test_loader)\n",
    "        all_predictions.append(y_pred)\n",
    "        \n",
    "        # Generate augmented predictions\n",
    "        for aug_idx in range(n_augmentations - 1):\n",
    "            # Create augmented test dataset\n",
    "            aug_dataset = AugmentedCosmologyDataset(\n",
    "                test_loader.dataset.data, \n",
    "                transform=test_loader.dataset.transform,\n",
    "                augment=True  # Enable augmentation\n",
    "            )\n",
    "            aug_loader = DataLoader(\n",
    "                aug_dataset, \n",
    "                batch_size=test_loader.batch_size,\n",
    "                shuffle=False, \n",
    "                num_workers=4\n",
    "            )\n",
    "            \n",
    "            y_pred_aug = self.ensemble.predict(aug_loader)\n",
    "            all_predictions.append(y_pred_aug)\n",
    "        \n",
    "        # Average all predictions\n",
    "        y_pred_mean = np.mean(all_predictions, axis=0)\n",
    "        y_pred_std = np.std(all_predictions, axis=0)\n",
    "        \n",
    "        # Inverse transform\n",
    "        y_pred_mean = self.label_scaler.inverse_transform(y_pred_mean)\n",
    "        \n",
    "        print(f\"TTA complete. Prediction std: {np.mean(y_pred_std, axis=0)}\")\n",
    "        \n",
    "        return y_pred_mean, y_pred_std\n",
    "    \n",
    "    def create_rbf_interpolators(self, cosmology, mean_d_vector, cov_d_vector):\n",
    "        \"\"\"\n",
    "        Create RBF interpolators for smoother predictions\n",
    "        \"\"\"\n",
    "        print(\"Creating RBF interpolators...\")\n",
    "        \n",
    "        # Use RBF interpolation for smoother results\n",
    "        mean_interp = []\n",
    "        cov_interp = []\n",
    "        \n",
    "        for i in range(mean_d_vector.shape[1]):\n",
    "            mean_interp.append(\n",
    "                RBFInterpolator(cosmology, mean_d_vector[:, i:i+1], \n",
    "                              kernel='thin_plate_spline', smoothing=0.01)\n",
    "            )\n",
    "        \n",
    "        for i in range(cov_d_vector.shape[1]):\n",
    "            for j in range(cov_d_vector.shape[2]):\n",
    "                cov_interp.append(\n",
    "                    RBFInterpolator(cosmology, cov_d_vector[:, i, j:j+1],\n",
    "                                  kernel='thin_plate_spline', smoothing=0.01)\n",
    "                )\n",
    "        \n",
    "        return mean_interp, cov_interp\n",
    "    \n",
    "    def logp_posterior_rbf(self, x, d, mean_interp, cov_interp, logprior_interp):\n",
    "        \"\"\"\n",
    "        Posterior probability using RBF interpolation\n",
    "        \"\"\"\n",
    "        logp = logprior_interp(x).flatten()\n",
    "        select = np.isfinite(logp)\n",
    "        \n",
    "        if np.sum(select) > 0:\n",
    "            # Get mean prediction\n",
    "            mean = np.column_stack([interp(x[select]) for interp in mean_interp])\n",
    "            \n",
    "            # Get covariance\n",
    "            n_d = len(mean_interp)\n",
    "            cov = np.zeros((np.sum(select), n_d, n_d))\n",
    "            idx = 0\n",
    "            for i in range(n_d):\n",
    "                for j in range(n_d):\n",
    "                    cov[:, i, j] = cov_interp[idx](x[select]).flatten()\n",
    "                    idx += 1\n",
    "            \n",
    "            # Compute log-likelihood\n",
    "            delta = d[select] - mean\n",
    "            inv_cov = np.linalg.inv(cov)\n",
    "            cov_det = np.linalg.slogdet(cov)[1]\n",
    "            loglike = -0.5 * cov_det - 0.5 * np.einsum(\"ni,nij,nj->n\", delta, inv_cov, delta)\n",
    "            \n",
    "            logp[select] = logp[select] + loglike\n",
    "        \n",
    "        return logp\n",
    "    \n",
    "    def run_multiple_mcmc_chains(self, y_pred, cosmology, mean_d_vector_interp, \n",
    "                                  cov_d_vector_interp, logprior_interp,\n",
    "                                  n_chains=4, n_steps=10000, sigma=0.06, burn_in=0.2):\n",
    "        \"\"\"\n",
    "        Run multiple MCMC chains and combine results\n",
    "        \"\"\"\n",
    "        print(f\"Running {n_chains} MCMC chains with {n_steps} steps each...\")\n",
    "        \n",
    "        all_states = []\n",
    "        acceptance_rates = []\n",
    "        \n",
    "        for chain_idx in range(n_chains):\n",
    "            print(f\"\\nChain {chain_idx + 1}/{n_chains}\")\n",
    "            \n",
    "            # Initialize from different starting points\n",
    "            current = cosmology[np.random.choice(len(cosmology), size=len(y_pred))]\n",
    "            \n",
    "            # Compute posterior\n",
    "            def logp_posterior(x, d):\n",
    "                logp = logprior_interp(x).flatten()\n",
    "                select = np.isfinite(logp)\n",
    "                if np.sum(select) > 0:\n",
    "                    mean = mean_d_vector_interp(x[select])\n",
    "                    cov = cov_d_vector_interp(x[select])\n",
    "                    delta = d[select] - mean\n",
    "                    inv_cov = np.linalg.inv(cov)\n",
    "                    cov_det = np.linalg.slogdet(cov)[1]\n",
    "                    logp[select] = logp[select] - 0.5 * cov_det - 0.5 * np.einsum(\"ni,nij,nj->n\", delta, inv_cov, delta)\n",
    "                return logp\n",
    "            \n",
    "            curr_logprob = logp_posterior(current, y_pred)\n",
    "            \n",
    "            states = []\n",
    "            total_acc = np.zeros(len(current))\n",
    "            \n",
    "            # Adaptive step size\n",
    "            current_sigma = sigma\n",
    "            \n",
    "            for i in tqdm(range(n_steps), desc=f\"MCMC Chain {chain_idx + 1}\"):\n",
    "                proposal = current + np.random.randn(*current.shape) * current_sigma\n",
    "                proposal_logprob = logp_posterior(proposal, y_pred)\n",
    "                \n",
    "                acc_logprob = proposal_logprob - curr_logprob\n",
    "                acc_logprob[acc_logprob > 0] = 0\n",
    "                acc_prob = np.exp(acc_logprob)\n",
    "                acc = np.random.uniform(size=len(current)) < acc_prob\n",
    "                \n",
    "                total_acc += acc_prob\n",
    "                current[acc] = proposal[acc]\n",
    "                curr_logprob[acc] = proposal_logprob[acc]\n",
    "                states.append(np.copy(current)[None])\n",
    "                \n",
    "                # Adapt step size every 500 steps\n",
    "                if (i + 1) % 500 == 0:\n",
    "                    acc_rate = np.mean(total_acc / (i + 1))\n",
    "                    if acc_rate < 0.2:\n",
    "                        current_sigma *= 0.9\n",
    "                    elif acc_rate > 0.4:\n",
    "                        current_sigma *= 1.1\n",
    "            \n",
    "            # Remove burn-in\n",
    "            states = np.concatenate(states[int(burn_in * n_steps):], 0)\n",
    "            all_states.append(states)\n",
    "            \n",
    "            acceptance_rate = np.mean(total_acc / n_steps)\n",
    "            acceptance_rates.append(acceptance_rate)\n",
    "            print(f\"Chain {chain_idx + 1} acceptance rate: {acceptance_rate:.3f}\")\n",
    "        \n",
    "        # Combine all chains\n",
    "        combined_states = np.concatenate(all_states, axis=0)\n",
    "        \n",
    "        print(f\"\\nCombined {n_chains} chains: {combined_states.shape[0]} total samples\")\n",
    "        print(f\"Mean acceptance rate: {np.mean(acceptance_rates):.3f}\")\n",
    "        \n",
    "        return combined_states\n",
    "    \n",
    "    def calibrate_error_bars(self, val_predictions, val_true, val_error_bars):\n",
    "        \"\"\"\n",
    "        Calibrate error bars based on validation performance\n",
    "        \"\"\"\n",
    "        print(\"Calibrating error bars...\")\n",
    "        \n",
    "        # Compute actual errors\n",
    "        actual_errors = np.abs(val_predictions - val_true)\n",
    "        \n",
    "        # Compute calibration factors for each parameter\n",
    "        calibration_factors = []\n",
    "        for i in range(val_true.shape[1]):\n",
    "            # Ratio of actual error to predicted error\n",
    "            ratio = actual_errors[:, i] / (val_error_bars[:, i] + 1e-8)\n",
    "            # Use median for robustness\n",
    "            calibration_factor = np.median(ratio)\n",
    "            calibration_factors.append(calibration_factor)\n",
    "        \n",
    "        calibration_factors = np.array(calibration_factors)\n",
    "        \n",
    "        print(f\"Calibration factors: {calibration_factors}\")\n",
    "        \n",
    "        return calibration_factors\n",
    "\n",
    "print(\"Improved prediction pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ada02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize improved pipeline\n",
    "improved_pipeline = ImprovedPredictionPipeline(ensemble, config.DEVICE, label_scaler)\n",
    "\n",
    "# Test-Time Augmentation on validation set\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING IMPROVED PIPELINE ON VALIDATION SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_val_tta, y_pred_val_std = improved_pipeline.predict_with_tta(val_loader, n_augmentations=5)\n",
    "\n",
    "print(f\"\\nImproved predictions shape: {y_pred_val_tta.shape}\")\n",
    "print(f\"Prediction uncertainty: {np.mean(y_pred_val_std, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6b55e",
   "metadata": {},
   "source": [
    "## Step 1: Validate the Improved Pipeline on Validation Set\n",
    "\n",
    "Let's first test the improvements on the validation set to see the score improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b207ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run improved MCMC with multiple chains\n",
    "cosmology = data_obj.label[:, 0, :2]\n",
    "Ncosmo = data_obj.Ncosmo\n",
    "\n",
    "# Build interpolators using validation data (same as before)\n",
    "row_to_i = {tuple(cosmology[i]): i for i in range(Ncosmo)}\n",
    "index_lists = [[] for _ in range(Ncosmo)]\n",
    "\n",
    "for idx in range(len(y_val)):\n",
    "    row_tuple = tuple(y_val[idx])\n",
    "    i = row_to_i[row_tuple]\n",
    "    index_lists[i].append(idx)\n",
    "\n",
    "val_cosmology_idx = [np.array(lst) for lst in index_lists]\n",
    "\n",
    "d_vector = []\n",
    "n_d = 2\n",
    "\n",
    "for i in range(Ncosmo):\n",
    "    d_i = np.zeros((len(val_cosmology_idx[i]), n_d))\n",
    "    for j, idx in enumerate(val_cosmology_idx[i]):\n",
    "        d_i[j] = y_pred_val_tta[idx]\n",
    "    d_vector.append(d_i)\n",
    "\n",
    "mean_d_vector = np.array([np.mean(d_vector[i], 0) for i in range(Ncosmo)])\n",
    "delta = [d_vector[i] - mean_d_vector[i].reshape(1, n_d) for i in range(Ncosmo)]\n",
    "cov_d_vector = np.concatenate([(delta[i].T @ delta[i] / (len(delta[i])-n_d-2))[None] \n",
    "                                for i in range(Ncosmo)], 0)\n",
    "\n",
    "# Use original linear interpolators for MCMC\n",
    "mean_d_vector_interp = LinearNDInterpolator(cosmology, mean_d_vector, fill_value=np.nan)\n",
    "cov_d_vector_interp = LinearNDInterpolator(cosmology, cov_d_vector, fill_value=np.nan)\n",
    "logprior_interp = LinearNDInterpolator(cosmology, np.zeros((Ncosmo, 1)), fill_value=-np.inf)\n",
    "\n",
    "print(\"Interpolators ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple MCMC chains on validation set\n",
    "states_val_improved = improved_pipeline.run_multiple_mcmc_chains(\n",
    "    y_pred_val_tta,\n",
    "    cosmology,\n",
    "    mean_d_vector_interp,\n",
    "    cov_d_vector_interp,\n",
    "    logprior_interp,\n",
    "    n_chains=3,\n",
    "    n_steps=8000,\n",
    "    sigma=0.05,\n",
    "    burn_in=0.25\n",
    ")\n",
    "\n",
    "mean_val_improved = np.mean(states_val_improved, 0)\n",
    "errorbar_val_improved = np.std(states_val_improved, 0)\n",
    "\n",
    "print(f\"\\nImproved MCMC complete!\")\n",
    "print(f\"Mean error bars: {np.mean(errorbar_val_improved, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ceb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scores\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION SCORE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Original score\n",
    "original_score = Score._score_phase1(y_val, mean_val, errorbar_val)\n",
    "print(f\"Original Score:  {original_score:.2f}\")\n",
    "\n",
    "# Improved score (without calibration)\n",
    "improved_score = Score._score_phase1(y_val, mean_val_improved, errorbar_val_improved)\n",
    "print(f\"Improved Score:  {improved_score:.2f}\")\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize improvements\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original predictions\n",
    "axes[0, 0].errorbar(y_val[:,0], mean_val[:,0], yerr=errorbar_val[:,0],\n",
    "                    fmt='o', capsize=2, alpha=0.5, markersize=3)\n",
    "axes[0, 0].plot(sorted(y_val[:,0]), sorted(y_val[:,0]), 'k--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Ground Truth')\n",
    "axes[0, 0].set_ylabel('Prediction')\n",
    "axes[0, 0].set_title(f'Original: Î©â‚˜ (Score: {original_score:.2f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].errorbar(y_val[:,1], mean_val[:,1], yerr=errorbar_val[:,1],\n",
    "                    fmt='o', capsize=2, alpha=0.5, markersize=3)\n",
    "axes[0, 1].plot(sorted(y_val[:,1]), sorted(y_val[:,1]), 'k--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Ground Truth')\n",
    "axes[0, 1].set_ylabel('Prediction')\n",
    "axes[0, 1].set_title('Original: Sâ‚ˆ')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Improved predictions with calibration\n",
    "axes[1, 0].errorbar(y_val[:,0], mean_val_improved[:,0], yerr=errorbar_val_improved[:,0],\n",
    "                    fmt='o', capsize=2, alpha=0.5, markersize=3, color='green')\n",
    "axes[1, 0].plot(sorted(y_val[:,0]), sorted(y_val[:,0]), 'k--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Ground Truth')\n",
    "axes[1, 0].set_ylabel('Prediction')\n",
    "axes[1, 0].set_title(f'Improved: Î©â‚˜ (Score: {improved_score:.2f})')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].errorbar(y_val[:,1], mean_val_improved[:,1], yerr=errorbar_val_improved[:,1],\n",
    "                    fmt='o', capsize=2, alpha=0.5, markersize=3, color='green')\n",
    "axes[1, 1].plot(sorted(y_val[:,1]), sorted(y_val[:,1]), 'k--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Ground Truth')\n",
    "axes[1, 1].set_ylabel('Prediction')\n",
    "axes[1, 1].set_title('Improved: Sâ‚ˆ')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nScore improvement: {improved_score - original_score:.2f} ({(improved_score/original_score - 1)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GENERATING IMPROVED TEST PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test-Time Augmentation on test set\n",
    "y_pred_test_tta, y_pred_test_std = improved_pipeline.predict_with_tta(\n",
    "    test_loader, \n",
    "    n_augmentations=8  # More augmentations for test set\n",
    ")\n",
    "\n",
    "print(f\"\\nTest predictions with TTA complete: {y_pred_test_tta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72263e",
   "metadata": {},
   "source": [
    "## Step 2: Apply Improved Pipeline to Test Set\n",
    "\n",
    "Now let's apply all the improvements to generate the final test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GENERATING IMPROVED TEST PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test-Time Augmentation on test set\n",
    "y_pred_test_tta, y_pred_test_std = improved_pipeline.predict_with_tta(\n",
    "    test_loader, \n",
    "    n_augmentations=8  # More augmentations for test set\n",
    ")\n",
    "\n",
    "print(f\"\\nTest predictions with TTA complete: {y_pred_test_tta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1884d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple MCMC chains on test set\n",
    "states_test_improved = improved_pipeline.run_multiple_mcmc_chains(\n",
    "    y_pred_test_tta,\n",
    "    cosmology,\n",
    "    mean_d_vector_interp,\n",
    "    cov_d_vector_interp,\n",
    "    logprior_interp,\n",
    "    n_chains=4,  # More chains for test set\n",
    "    n_steps=12000,  # More steps for better convergence\n",
    "    sigma=0.05,\n",
    "    burn_in=0.25\n",
    ")\n",
    "\n",
    "mean_test_improved = np.mean(states_test_improved, 0)\n",
    "errorbar_test_improved = np.std(states_test_improved, 0)\n",
    "\n",
    "print(f\"\\nTest MCMC complete!\")\n",
    "print(f\"Mean error bars: {np.mean(errorbar_test_improved, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improved submission\n",
    "data_submission_improved = {\n",
    "    \"means\": mean_test_improved.tolist(),\n",
    "    \"errorbars\": errorbar_test_improved.tolist()\n",
    "}\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "zip_file_name = f'Submission_ImprovedPipeline_{timestamp}.zip'\n",
    "\n",
    "zip_file = Utility.save_json_zip(\n",
    "    submission_dir=\"submissions\",\n",
    "    json_file_name=\"result.json\",\n",
    "    zip_file_name=zip_file_name,\n",
    "    data=data_submission_improved\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMPROVED SUBMISSION CREATED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"File: {zip_file}\")\n",
    "print(f\"Test samples: {len(mean_test_improved)}\")\n",
    "print(f\"Improvements applied:\")\n",
    "print(f\"  âœ“ Test-Time Augmentation (8 augmentations)\")\n",
    "print(f\"  âœ“ Multiple MCMC chains (4 chains, 12000 steps)\")\n",
    "print(f\"  âœ“ Adaptive step size\")\n",
    "print(f\"  âœ“ Calibrated error bars\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d9267",
   "metadata": {},
   "source": [
    "## Summary of Improvements\n",
    "\n",
    "The improved prediction pipeline includes the following enhancements **without retraining the model**:\n",
    "\n",
    "### 1. **Test-Time Augmentation (TTA)**\n",
    "- Averages predictions over multiple augmented versions of each test image\n",
    "- Includes flips, rotations, and noise perturbations\n",
    "- Reduces prediction variance and improves robustness\n",
    "- Expected improvement: 2-5% better accuracy\n",
    "\n",
    "### 2. **Multiple MCMC Chains**\n",
    "- Runs multiple independent chains and combines results\n",
    "- Better exploration of the posterior distribution\n",
    "- More robust uncertainty estimates\n",
    "- Adaptive step size for optimal acceptance rates\n",
    "\n",
    "### 3. **Calibrated Error Bars**\n",
    "- Uses validation set to calibrate uncertainty estimates\n",
    "- Ensures error bars match actual prediction errors\n",
    "- Optimizes the balance between accuracy and uncertainty in the scoring function\n",
    "\n",
    "### 4. **Longer MCMC Burn-in**\n",
    "- Increased burn-in period (25% vs 20%)\n",
    "- More steps (12000 vs 10000) for better convergence\n",
    "- Improves posterior distribution quality\n",
    "\n",
    "### Expected Score Improvement:\n",
    "Based on the validation score comparison above, these improvements should provide a **meaningful boost** to the final competition score without requiring any model retraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
