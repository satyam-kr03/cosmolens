{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a956880-e2b9-4868-8446-ad66302d9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    @staticmethod\n",
    "    def add_noise(data, mask, ng, pixel_size=2.):\n",
    "        \"\"\"\n",
    "        Add noise to a noiseless convergence map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array\n",
    "            Noiseless convergence maps.\n",
    "        mask : np.array\n",
    "            Binary mask map.\n",
    "        ng : float\n",
    "            Number of galaxies per arcmin². This determines the noise level; a larger number means smaller noise.\n",
    "        pixel_size : float, optional\n",
    "            Pixel size in arcminutes (default is 2.0).\n",
    "        \"\"\"\n",
    "\n",
    "        return data + np.random.randn(*data.shape) * 0.4 / (2*ng*pixel_size**2)**0.5 * mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_np(data_dir, file_name):\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        return np.load(file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_np(data_dir, file_name, data):\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        np.save(file_path, data)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_json_zip(submission_dir, json_file_name, zip_file_name, data):\n",
    "        \"\"\"\n",
    "        Save a dictionary with 'means' and 'errorbars' into a JSON file,\n",
    "        then compress it into a ZIP file inside submission_dir.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        submission_dir : str\n",
    "            Path to the directory where the ZIP file will be saved.\n",
    "        file_name : str\n",
    "            Name of the ZIP file (without extension).\n",
    "        data : dict\n",
    "            Dictionary with keys 'means' and 'errorbars'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to the created ZIP file.\n",
    "        \"\"\"\n",
    "        os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "        json_path = os.path.join(submission_dir, json_file_name)\n",
    "\n",
    "        # Save JSON file\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "        # Path to ZIP\n",
    "        zip_path = os.path.join(submission_dir, zip_file_name)\n",
    "\n",
    "        # Create ZIP containing only the JSON\n",
    "        with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "            zf.write(json_path, arcname=json_file_name)\n",
    "\n",
    "        # Remove the standalone JSON after zipping\n",
    "        os.remove(json_path)\n",
    "\n",
    "        return zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, data_dir, USE_PUBLIC_DATASET):\n",
    "        self.USE_PUBLIC_DATASET = USE_PUBLIC_DATASET\n",
    "        self.data_dir = data_dir\n",
    "        self.mask_file = 'WIDE12H_bin2_2arcmin_mask.npy'\n",
    "        self.viz_label_file = 'label.npy'\n",
    "        if self.USE_PUBLIC_DATASET:\n",
    "            self.kappa_file = 'WIDE12H_bin2_2arcmin_kappa.npy'\n",
    "            self.label_file = self.viz_label_file\n",
    "            self.Ncosmo = 101  # Number of cosmologies in the entire training data\n",
    "            self.Nsys = 256    # Number of systematic realizations in the entire training data\n",
    "            self.test_kappa_file = 'WIDE12H_bin2_2arcmin_kappa_noisy_test.npy'\n",
    "            self.Ntest = 4000  # Number of instances in the test data\n",
    "        else:\n",
    "            self.kappa_file = 'sampled_WIDE12H_bin2_2arcmin_kappa.npy'\n",
    "            self.label_file = 'sampled_label.npy'\n",
    "            self.Ncosmo = 3    # Number of cosmologies in the sampled training data\n",
    "            self.Nsys = 30     # Number of systematic realizations in the sampled training data\n",
    "            self.test_kappa_file = 'sampled_WIDE12H_bin2_2arcmin_kappa_noisy_test.npy'\n",
    "            self.Ntest = 3     # Number of instances in the sampled test data\n",
    "        \n",
    "        self.shape = [1424,176] # dimensions of each map \n",
    "        self.pixelsize_arcmin = 2 # pixel size in arcmin\n",
    "        self.pixelsize_radian = self.pixelsize_arcmin / 60 / 180 * np.pi # pixel size in radian\n",
    "        self.ng = 30  # galaxy number density. This determines the noise level of the experiment. Do not change this number.\n",
    "\n",
    "    def load_train_data(self):\n",
    "        self.mask = Utility.load_np(data_dir=self.data_dir, file_name=self.mask_file) # A binary map that shows which parts of the sky are observed and which areas are blocked\n",
    "        self.kappa = np.zeros((self.Ncosmo, self.Nsys, *self.shape), dtype=np.float16)\n",
    "        self.kappa[:,:,self.mask] = Utility.load_np(data_dir=self.data_dir, file_name=self.kappa_file) # Training convergence maps\n",
    "        self.label = Utility.load_np(data_dir=self.data_dir, file_name=self.label_file) # Training labels (cosmological and physical paramameters) of each training map\n",
    "        self.viz_label = Utility.load_np(data_dir=self.data_dir, file_name=self.viz_label_file) # For visualization of parameter distributions\n",
    "\n",
    "    def load_test_data(self):\n",
    "        self.kappa_test = np.zeros((self.Ntest, *self.shape), dtype=np.float16)\n",
    "        self.kappa_test[:,self.mask] = Utility.load_np(data_dir=self.data_dir, file_name=self.test_kappa_file) # Test noisy convergence maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization:\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_mask(mask):\n",
    "        plt.figure(figsize=(30,100))\n",
    "        plt.imshow(mask.T)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_noiseless_training_convergence_map(kappa):\n",
    "        plt.figure(figsize=(30,100))\n",
    "        plt.imshow(kappa[0,0].T, vmin=-0.02, vmax=0.07)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_noisy_training_convergence_map(kappa, mask, pixelsize_arcmin, ng):\n",
    "        plt.figure(figsize=(30,100))\n",
    "        plt.imshow(Utility.add_noise(kappa[0,0], mask, ng, pixelsize_arcmin).T, vmin=-0.02, vmax=0.07)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_cosmological_parameters_OmegaM_S8(label):\n",
    "        plt.scatter(label[:,0,0], label[:,0,1])\n",
    "        plt.xlabel(r'$\\Omega_m$')\n",
    "        plt.ylabel(r'$S_8$')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_baryonic_physics_parameters(label):\n",
    "        plt.scatter(label[0,:,2], label[0,:,3])\n",
    "        plt.xlabel(r'$T_{\\mathrm{AGN}}$')\n",
    "        plt.ylabel(r'$f_0$')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_photometric_redshift_uncertainty_parameters(label):\n",
    "        plt.hist(label[0,:,4], bins=20)\n",
    "        plt.xlabel(r'$\\Delta z$')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score:\n",
    "    @staticmethod\n",
    "    def _score_phase1(true_cosmo, infer_cosmo, errorbar):\n",
    "        \"\"\"\n",
    "        Computes the log-likelihood score for Phase 1 based on predicted cosmological parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        true_cosmo : np.ndarray\n",
    "            Array of true cosmological parameters (shape: [n_samples, n_params]).\n",
    "        infer_cosmo : np.ndarray\n",
    "            Array of inferred cosmological parameters from the model (same shape as true_cosmo).\n",
    "        errorbar : np.ndarray\n",
    "            Array of standard deviations (uncertainties) for each inferred parameter \n",
    "            (same shape as true_cosmo).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of scores for each sample (shape: [n_samples]).\n",
    "        \"\"\"\n",
    "        \n",
    "        sq_error = (true_cosmo - infer_cosmo)**2\n",
    "        scale_factor = 1000  # This is a constant that scales the error term.\n",
    "        score = - np.sum(sq_error / errorbar**2 + np.log(errorbar**2) + scale_factor * sq_error, 1)\n",
    "        score = np.mean(score)\n",
    "        if score >= -10**6: # Set a minimum of the score (to properly display on Codabench)\n",
    "            return score\n",
    "        else:\n",
    "            return -10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56be65b-19a7-4353-a752-c07d2512c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069869cd-9501-47a5-8c32-c8f3a7f25756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_PUBLIC_DATASET = False\n",
    "\n",
    "USE_PUBLIC_DATASET = True\n",
    "PUBLIC_DATA_DIR = '../data'  # This is only required when you set USE_PUBLIC_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be959a29-b52e-4e8e-b20e-e6f2c507b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_PUBLIC_DATASET:                                         # Testing this startking kit with a tiny sample of the training data (3, 30, 1424, 176)\n",
    "    DATA_DIR = os.path.join(root_dir, 'input_data/')\n",
    "else:                                                              # Training your model with all training data (101, 256, 1424, 176)\n",
    "    DATA_DIR = PUBLIC_DATA_DIR    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4be679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data class object\n",
    "data_obj = Data(data_dir=DATA_DIR, USE_PUBLIC_DATASET=USE_PUBLIC_DATASET)\n",
    "\n",
    "# Load train data\n",
    "data_obj.load_train_data()\n",
    "\n",
    "# Load test data\n",
    "data_obj.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475663b-ca7c-4443-af4a-e1e3eb416a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncosmo = data_obj.Ncosmo\n",
    "Nsys = data_obj.Nsys\n",
    "\n",
    "print(f'There are {Ncosmo} cosmological models, each has {Nsys} realizations of nuisance parameters in the training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8d763-8a2e-477f-8b83-061e2717004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of the training data = {data_obj.kappa.shape}')\n",
    "print(f'Shape of the mask = {data_obj.mask.shape}')\n",
    "print(f'Shape of the training label = {data_obj.label.shape}')\n",
    "print(f'Shape of the test data = {data_obj.kappa_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff43f8e-f8e8-485d-ba56-24eb82293ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# np.random.seed(31415)\n",
    "# Ncosmo = data_obj.kappa.shape[0]\n",
    "# Nsys = data_obj.kappa.shape[1]\n",
    "\n",
    "# # Initialize output array\n",
    "# noisy_kappa = np.zeros(data_obj.kappa.shape, dtype=np.float32)\n",
    "\n",
    "# # Process one cosmology at a time to minimize memory usage\n",
    "# for cosmo_idx in tqdm(range(Ncosmo), desc=\"Adding noise\"):\n",
    "#     # Process this entire cosmology\n",
    "#     cosmo_data = data_obj.kappa[cosmo_idx].astype(np.float64)\n",
    "    \n",
    "#     noisy_kappa[cosmo_idx] = Utility.add_noise(\n",
    "#         data=cosmo_data,\n",
    "#         mask=data_obj.mask,\n",
    "#         ng=data_obj.ng,\n",
    "#         pixel_size=data_obj.pixelsize_arcmin\n",
    "#     ).astype(np.float32)\n",
    "    \n",
    "#     # Clean up\n",
    "#     del cosmo_data\n",
    "    \n",
    "#     # Force garbage collection every 10 cosmologies\n",
    "#     if (cosmo_idx + 1) % 10 == 0:\n",
    "#         gc.collect()\n",
    "\n",
    "# print(\"Noise addition complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0a352-8abd-41f2-a595-632fdb7c1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Shape of the noised data {noisy_kappa.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5ba29-70db-48c5-8c4f-98709ba6779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training and validation sets\n",
    "\n",
    "# NP_idx = np.arange(Nsys)  # The indices of Nsys nuisance parameter realizations\n",
    "# split_fraction = 0.2      # Set the fraction of data you want to split (between 0 and 1)\n",
    "# seed = 5566               # Define your random seed for reproducible results\n",
    "\n",
    "# train_NP_idx, val_NP_idx = train_test_split(NP_idx, test_size=split_fraction,\n",
    "#                                             random_state=seed)\n",
    "\n",
    "# noisy_kappa_train = noisy_kappa[:, train_NP_idx]      # shape = (Ncosmo, len(train_NP_idx), 1424, 176)\n",
    "# label_train = data_obj.label[:, train_NP_idx]         # shape = (Ncosmo, len(train_NP_idx), 5)\n",
    "# noisy_kappa_val = noisy_kappa[:, val_NP_idx]          # shape = (Ncosmo, len(val_NP_idx), 1424, 176)\n",
    "# label_val = data_obj.label[:, val_NP_idx]             # shape = (Ncosmo, len(val_NP_idx), 5)\n",
    "\n",
    "# Ntrain = label_train.shape[0]*label_train.shape[1]\n",
    "# Nval = label_val.shape[0]*label_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d127d-000e-472c-9b88-2ca1e182ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Shape of the split training data = {noisy_kappa_train.shape}')\n",
    "# print(f'Shape of the split validation data = {noisy_kappa_val.shape}')\n",
    "\n",
    "# print(f'Shape of the split training labels = {label_train.shape}')\n",
    "# print(f'Shape of the split validation labels = {label_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c5214-1213-46ae-9431-f71433f27b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the split data and labels for future usage\n",
    "\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_train.npy\",data=noisy_kappa_train)\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"label_train.npy\",data=label_train)\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_val.npy\",data=noisy_kappa_val)\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"label_val.npy\",data=label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8bb2f-0acf-48bc-90d5-58e0c1e38709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved split data (if you saved it at DATA_DIR before)\n",
    "\n",
    "noisy_kappa_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_train.npy\")\n",
    "label_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_train.npy\")\n",
    "noisy_kappa_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_val.npy\")\n",
    "label_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_val.npy\")\n",
    "\n",
    "Ntrain = label_train.shape[0]*label_train.shape[1]\n",
    "Nval = label_val.shape[0]*label_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b77f1c-1c2b-46ef-9247-147f914fc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for CNN\n",
    "X_train = noisy_kappa_train.reshape(Ntrain, *data_obj.shape)\n",
    "X_val = noisy_kappa_val.reshape(Nval, *data_obj.shape)\n",
    "\n",
    "# Here, we ignore the nuisance parameters and only keep the 2 cosmological parameters\n",
    "y_train = label_train.reshape(Ntrain, 5)[:, :2]\n",
    "y_val = label_val.reshape(Nval, 5)[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad216b-9db5-4ea4-ac26-48004b611e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of the split training data = {X_train.shape}')\n",
    "print(f'Shape of the split validation data = {X_val.shape}')\n",
    "\n",
    "print(f'Shape of the split training labels = {y_train.shape}')\n",
    "print(f'Shape of the split validation labels = {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a03109-f9a6-47bb-ad65-0cace5a13c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask\n",
    "Visualization.plot_mask(mask=data_obj.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dea4b1-7c83-49bc-a7f2-3b99767cb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noiseless training convergence map\n",
    "Visualization.plot_noiseless_training_convergence_map(kappa=data_obj.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3116d-ecbc-4fa2-ba14-306da9320291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy training convergence map\n",
    "Visualization.plot_noisy_training_convergence_map(kappa=data_obj.kappa,\n",
    "                                                  mask=data_obj.mask,\n",
    "                                                  pixelsize_arcmin=data_obj.pixelsize_arcmin,\n",
    "                                                  ng=data_obj.ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487aee0-98c8-4b05-9642-9f4157f9831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.plot_cosmological_parameters_OmegaM_S8(label=data_obj.viz_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b2281-401e-4149-9b75-50ac97d50dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.plot_baryonic_physics_parameters(label=data_obj.viz_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b8b2e-b7e3-4fbd-907e-81a460049215",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.plot_photometric_redshift_uncertainty_parameters(label=data_obj.viz_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa451c18-7541-400a-9d68-9308d99bd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your path for saving the trained model\n",
    "MODEL_SAVE_PATH = os.path.join(root_dir, \"Phase1_starting_kit_CNN_MCMC_baseline.pth\")\n",
    "\n",
    "class Config:\n",
    "    IMG_HEIGHT = data_obj.shape[0]\n",
    "    IMG_WIDTH = data_obj.shape[1]\n",
    "    \n",
    "    # Parameters to predict (Omega_m, S_8)\n",
    "    NUM_TARGETS = 2\n",
    "\n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 2e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    DROPOUT1 = 0.2\n",
    "    DROPOUT2 = 0.1\n",
    "    \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    MODEL_SAVE_PATH = MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for CNN-RF Ensemble\n",
    "class EnsembleConfig:\n",
    "    IMG_HEIGHT = data_obj.shape[0]\n",
    "    IMG_WIDTH = data_obj.shape[1]\n",
    "    \n",
    "    # Parameters to predict (Omega_m, S_8)\n",
    "    NUM_TARGETS = 2\n",
    "\n",
    "    # Training hyperparameters for CNN\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 2e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    FEATURE_DIM = 128  # Dimension of CNN features\n",
    "    DROPOUT1 = 0.2\n",
    "    DROPOUT2 = 0.1\n",
    "    \n",
    "    # Random Forest hyperparameters\n",
    "    RF_PARAMS = {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    CNN_MODEL_SAVE_PATH = os.path.join(root_dir, \"Phase1_CNN_feature_extractor.pth\")\n",
    "    RF_MODEL_SAVE_PATH = os.path.join(root_dir, \"Phase1_RF_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ensemble Configuration and Model\n",
    "ensemble_config = EnsembleConfig()\n",
    "print(f\"Using device: {ensemble_config.DEVICE}\")\n",
    "print(f\"CNN Feature dimension: {ensemble_config.FEATURE_DIM}\")\n",
    "print(f\"Random Forest params: {ensemble_config.RF_PARAMS}\")\n",
    "\n",
    "# Initialize CNN Feature Extractor with auxiliary prediction head\n",
    "cnn_feature_model = CNN_FeatureExtractor_WithHead(\n",
    "    height=ensemble_config.IMG_HEIGHT,\n",
    "    width=ensemble_config.IMG_WIDTH,\n",
    "    num_targets=ensemble_config.NUM_TARGETS,\n",
    "    feature_dim=ensemble_config.FEATURE_DIM,\n",
    "    dropout1=ensemble_config.DROPOUT1,\n",
    "    dropout2=ensemble_config.DROPOUT2\n",
    ").to(ensemble_config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cfd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN-RF Ensemble\n",
    "\n",
    "USE_PRETRAINED_ENSEMBLE = False\n",
    "# USE_PRETRAINED_ENSEMBLE = True\n",
    "\n",
    "if not USE_PRETRAINED_ENSEMBLE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Training CNN Feature Extractor\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Train the CNN feature extractor with auxiliary prediction head\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(cnn_feature_model.parameters(),\n",
    "                                lr=ensemble_config.LEARNING_RATE,\n",
    "                                weight_decay=ensemble_config.WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(ensemble_config.EPOCHS):\n",
    "        train_loss = train_epoch_ensemble(cnn_feature_model, train_loader, loss_fn, optimizer, ensemble_config.DEVICE)\n",
    "        val_loss = validate_epoch_ensemble(cnn_feature_model, val_loader, loss_fn, ensemble_config.DEVICE)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{ensemble_config.EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(cnn_feature_model.state_dict(), ensemble_config.CNN_MODEL_SAVE_PATH)\n",
    "            print(f\"  -> New best CNN model saved!\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nCNN training finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "    \n",
    "    # Load the best CNN model\n",
    "    cnn_feature_model.load_state_dict(torch.load(ensemble_config.CNN_MODEL_SAVE_PATH, weights_only=True))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Training Random Forest on CNN Features\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Extract the feature extractor (without prediction head)\n",
    "    feature_extractor = cnn_feature_model.get_feature_extractor()\n",
    "    \n",
    "    # Create ensemble with trained feature extractor\n",
    "    ensemble_model = CNN_RF_Ensemble(feature_extractor, rf_params=ensemble_config.RF_PARAMS)\n",
    "    \n",
    "    # Train Random Forest on extracted features\n",
    "    ensemble_model.fit_rf(train_loader)\n",
    "    \n",
    "    # Save Random Forest model\n",
    "    import pickle\n",
    "    with open(ensemble_config.RF_MODEL_SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump(ensemble_model.rf_model, f)\n",
    "    print(f\"Random Forest model saved to {ensemble_config.RF_MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Ensemble training complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    # Load pretrained models\n",
    "    import pickle\n",
    "    \n",
    "    if os.path.exists(ensemble_config.CNN_MODEL_SAVE_PATH) and os.path.exists(ensemble_config.RF_MODEL_SAVE_PATH):\n",
    "        print(\"Loading pretrained ensemble models...\")\n",
    "        \n",
    "        # Load CNN feature extractor\n",
    "        cnn_feature_model.load_state_dict(torch.load(ensemble_config.CNN_MODEL_SAVE_PATH, weights_only=True))\n",
    "        feature_extractor = cnn_feature_model.get_feature_extractor()\n",
    "        \n",
    "        # Create ensemble and load RF model\n",
    "        ensemble_model = CNN_RF_Ensemble(feature_extractor, rf_params=ensemble_config.RF_PARAMS)\n",
    "        with open(ensemble_config.RF_MODEL_SAVE_PATH, 'rb') as f:\n",
    "            ensemble_model.rf_model = pickle.load(f)\n",
    "        \n",
    "        print(\"Pretrained models loaded successfully!\")\n",
    "    else:\n",
    "        warning_msg = \"Pretrained ensemble models don't exist. Please train first.\"\n",
    "        warnings.warn(warning_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffef862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation predictions using CNN-RF Ensemble\n",
    "\n",
    "print(\"Making predictions on validation set using CNN-RF Ensemble...\")\n",
    "y_pred_val_ensemble = ensemble_model.predict(val_loader)\n",
    "\n",
    "# Inverse transform predictions back to original scale\n",
    "y_pred_val_ensemble = label_scaler.inverse_transform(y_pred_val_ensemble)\n",
    "\n",
    "print(f\"Validation predictions shape: {y_pred_val_ensemble.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of CNN-RF Ensemble predictions vs validation labels\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Omega_m plot\n",
    "axes[0].scatter(y_val[:,0], y_pred_val_ensemble[:,0], alpha=0.6, s=20)\n",
    "axes[0].plot(sorted(y_val[:,0]), sorted(y_val[:,0]),\n",
    "             color='grey', linestyle='dashed', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlim(np.min(y_val[:,0]), np.max(y_val[:,0]))\n",
    "axes[0].set_ylim(0, 0.7)\n",
    "axes[0].set_xlabel('Ground Truth', fontsize=12)\n",
    "axes[0].set_ylabel('Prediction', fontsize=12)\n",
    "axes[0].set_title(r'$\\Omega_m$ (CNN-RF Ensemble)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# S_8 plot\n",
    "axes[1].scatter(y_val[:,1], y_pred_val_ensemble[:,1], alpha=0.6, s=20)\n",
    "axes[1].plot(sorted(y_val[:,1]), sorted(y_val[:,1]),\n",
    "             color='grey', linestyle='dashed', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlim(np.min(y_val[:,1]), np.max(y_val[:,1]))\n",
    "axes[1].set_ylim(0.65, 1)\n",
    "axes[1].set_xlabel('Ground Truth', fontsize=12)\n",
    "axes[1].set_ylabel('Prediction', fontsize=12)\n",
    "axes[1].set_title(r'$S_8$ (CNN-RF Ensemble)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse_omega_m = mean_squared_error(y_val[:,0], y_pred_val_ensemble[:,0])\n",
    "mse_s8 = mean_squared_error(y_val[:,1], y_pred_val_ensemble[:,1])\n",
    "r2_omega_m = r2_score(y_val[:,0], y_pred_val_ensemble[:,0])\n",
    "r2_s8 = r2_score(y_val[:,1], y_pred_val_ensemble[:,1])\n",
    "\n",
    "print(f\"\\nValidation Metrics (CNN-RF Ensemble):\")\n",
    "print(f\"  Omega_m - MSE: {mse_omega_m:.6f}, R²: {r2_omega_m:.4f}\")\n",
    "print(f\"  S_8     - MSE: {mse_s8:.6f}, R²: {r2_s8:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3192ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare summary statistics for MCMC using ensemble predictions\n",
    "\n",
    "# Group validation indices by cosmology (same as before)\n",
    "cosmology = data_obj.label[:,0,:2]\n",
    "row_to_i = {tuple(cosmology[i]): i for i in range(Ncosmo)}\n",
    "index_lists = [[] for _ in range(cosmology.shape[0])]\n",
    "\n",
    "for idx in range(len(y_val)):\n",
    "    row_tuple = tuple(y_val[idx])\n",
    "    i = row_to_i[row_tuple]\n",
    "    index_lists[i].append(idx)\n",
    "\n",
    "val_cosmology_idx_ensemble = [np.array(lst) for lst in index_lists]\n",
    "\n",
    "# Summary statistics using ensemble predictions\n",
    "d_vector_ensemble = []\n",
    "n_d = 2\n",
    "\n",
    "for i in range(Ncosmo):\n",
    "    d_i = np.zeros((len(val_cosmology_idx_ensemble[i]), n_d))\n",
    "    for j, idx in enumerate(val_cosmology_idx_ensemble[i]):\n",
    "        d_i[j] = y_pred_val_ensemble[idx]\n",
    "    d_vector_ensemble.append(d_i)\n",
    "\n",
    "# Mean summary statistics\n",
    "mean_d_vector_ensemble = []\n",
    "for i in range(Ncosmo):\n",
    "    mean_d_vector_ensemble.append(np.mean(d_vector_ensemble[i], 0))\n",
    "mean_d_vector_ensemble = np.array(mean_d_vector_ensemble)\n",
    "\n",
    "# Covariance matrix\n",
    "delta_ensemble = []\n",
    "for i in range(Ncosmo):\n",
    "    delta_ensemble.append((d_vector_ensemble[i] - mean_d_vector_ensemble[i].reshape(1, n_d)))\n",
    "\n",
    "cov_d_vector_ensemble = [(delta_ensemble[i].T @ delta_ensemble[i] / (len(delta_ensemble[i])-n_d-2))[None] \n",
    "                          for i in range(Ncosmo)]\n",
    "cov_d_vector_ensemble = np.concatenate(cov_d_vector_ensemble, 0)\n",
    "\n",
    "print(\"Summary statistics computed for ensemble predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolators for MCMC with ensemble predictions\n",
    "\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "mean_d_vector_interp_ensemble = LinearNDInterpolator(cosmology, mean_d_vector_ensemble, fill_value=np.nan)\n",
    "cov_d_vector_interp_ensemble = LinearNDInterpolator(cosmology, cov_d_vector_ensemble, fill_value=np.nan)\n",
    "logprior_interp_ensemble = LinearNDInterpolator(cosmology, np.zeros((Ncosmo, 1)), fill_value=-np.inf)\n",
    "\n",
    "def log_prior_ensemble(x):\n",
    "    logprior = logprior_interp_ensemble(x).flatten()\n",
    "    return logprior\n",
    "\n",
    "def loglike_ensemble(x, d):\n",
    "    mean = mean_d_vector_interp_ensemble(x)\n",
    "    cov = cov_d_vector_interp_ensemble(x)\n",
    "    delta = d - mean\n",
    "    \n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    cov_det = np.linalg.slogdet(cov)[1]\n",
    "    \n",
    "    return -0.5 * cov_det - 0.5 * np.einsum(\"ni,nij,nj->n\", delta, inv_cov, delta)\n",
    "\n",
    "def logp_posterior_ensemble(x, d):\n",
    "    logp = log_prior_ensemble(x)\n",
    "    select = np.isfinite(logp)\n",
    "    if np.sum(select) > 0:\n",
    "        logp[select] = logp[select] + loglike_ensemble(x[select], d[select])\n",
    "    return logp\n",
    "\n",
    "print(\"MCMC interpolators created for ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sampling for validation set with ensemble predictions\n",
    "\n",
    "Nstep_ensemble = 10000\n",
    "sigma_ensemble = 0.06\n",
    "\n",
    "current_ensemble = cosmology[np.random.choice(Ncosmo, size=Nval)]\n",
    "curr_logprob_ensemble = logp_posterior_ensemble(current_ensemble, y_pred_val_ensemble)\n",
    "\n",
    "states_ensemble = []\n",
    "total_acc_ensemble = np.zeros(len(current_ensemble))\n",
    "\n",
    "print(\"Running MCMC for validation set with ensemble predictions...\")\n",
    "t = time.time()\n",
    "\n",
    "for i in range(Nstep_ensemble):\n",
    "    proposal = current_ensemble + np.random.randn(*current_ensemble.shape) * sigma_ensemble\n",
    "    proposal_logprob = logp_posterior_ensemble(proposal, y_pred_val_ensemble)\n",
    "    \n",
    "    acc_logprob = proposal_logprob - curr_logprob_ensemble\n",
    "    acc_logprob[acc_logprob > 0] = 0\n",
    "    \n",
    "    acc_prob = np.exp(acc_logprob)\n",
    "    acc = np.random.uniform(size=len(current_ensemble)) < acc_prob\n",
    "    \n",
    "    total_acc_ensemble += acc_prob\n",
    "    current_ensemble[acc] = proposal[acc]\n",
    "    curr_logprob_ensemble[acc] = proposal_logprob[acc]\n",
    "    \n",
    "    states_ensemble.append(np.copy(current_ensemble)[None])\n",
    "    \n",
    "    if i % (0.1*Nstep_ensemble) == 0.1*Nstep_ensemble-1:\n",
    "        print(\n",
    "            'Step:', len(states_ensemble),\n",
    "            'Time:', f'{time.time() - t:.2f}s',\n",
    "            'Min acceptance rate:', f'{np.min(total_acc_ensemble / (i + 1)):.4f}',\n",
    "            'Mean acceptance rate:', f'{np.mean(total_acc_ensemble / (i + 1)):.4f}'\n",
    "        )\n",
    "        t = time.time()\n",
    "\n",
    "# Remove burn-in\n",
    "states_ensemble = np.concatenate(states_ensemble[int(0.2*Nstep_ensemble):], 0)\n",
    "\n",
    "# Mean and std of samples\n",
    "mean_val_ensemble = np.mean(states_ensemble, 0)\n",
    "errorbar_val_ensemble = np.std(states_ensemble, 0)\n",
    "\n",
    "print(f\"\\nMCMC complete. Mean error bars: {np.mean(errorbar_val_ensemble, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of MCMC results with error bars (Ensemble)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Omega_m plot with error bars\n",
    "axes[0].errorbar(y_val[:,0], mean_val_ensemble[:,0], yerr=errorbar_val_ensemble[:,0],\n",
    "                 fmt='o', capsize=3, capthick=1, ecolor='grey', alpha=0.6, markersize=4)\n",
    "axes[0].plot(sorted(y_val[:,0]), sorted(y_val[:,0]),\n",
    "             color='grey', linestyle='dashed', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlim(np.min(y_val[:,0]), np.max(y_val[:,0]))\n",
    "axes[0].set_ylim(0, 0.7)\n",
    "axes[0].set_xlabel('Ground Truth', fontsize=12)\n",
    "axes[0].set_ylabel('Prediction', fontsize=12)\n",
    "axes[0].set_title(r'$\\Omega_m$ (CNN-RF Ensemble + MCMC)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# S_8 plot with error bars\n",
    "axes[1].errorbar(y_val[:,1], mean_val_ensemble[:,1], yerr=errorbar_val_ensemble[:,1],\n",
    "                 fmt='o', capsize=3, capthick=1, ecolor='grey', alpha=0.6, markersize=4)\n",
    "axes[1].plot(sorted(y_val[:,1]), sorted(y_val[:,1]),\n",
    "             color='grey', linestyle='dashed', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlim(np.min(y_val[:,1]), np.max(y_val[:,1]))\n",
    "axes[1].set_ylim(0.65, 1)\n",
    "axes[1].set_xlabel('Ground Truth', fontsize=12)\n",
    "axes[1].set_ylabel('Prediction', fontsize=12)\n",
    "axes[1].set_title(r'$S_8$ (CNN-RF Ensemble + MCMC)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate validation score for ensemble\n",
    "\n",
    "validation_score_ensemble = Score._score_phase1(\n",
    "    true_cosmo=y_val,\n",
    "    infer_cosmo=mean_val_ensemble,\n",
    "    errorbar=errorbar_val_ensemble\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('VALIDATION RESULTS (CNN-RF Ensemble)')\n",
    "print('=' * 60)\n",
    "print(f'Average score: {np.mean(validation_score_ensemble):.2f}')\n",
    "print(f'Average error bar (Omega_m): {np.mean(errorbar_val_ensemble[:, 0]):.6f}')\n",
    "print(f'Average error bar (S_8): {np.mean(errorbar_val_ensemble[:, 1]):.6f}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predictions using CNN-RF Ensemble\n",
    "\n",
    "print(\"Making predictions on test set using CNN-RF Ensemble...\")\n",
    "y_pred_test_ensemble = ensemble_model.predict(test_loader)\n",
    "\n",
    "# Inverse transform predictions back to original scale\n",
    "y_pred_test_ensemble = label_scaler.inverse_transform(y_pred_test_ensemble)\n",
    "\n",
    "print(f\"Test predictions shape: {y_pred_test_ensemble.shape}\")\n",
    "print(f\"Sample predictions (first 5):\\n{y_pred_test_ensemble[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sampling for test set with ensemble predictions\n",
    "\n",
    "Nstep_test = 10000\n",
    "sigma_test = 0.06\n",
    "\n",
    "current_test = cosmology[np.random.choice(Ncosmo, size=data_obj.Ntest)]\n",
    "curr_logprob_test = logp_posterior_ensemble(current_test, y_pred_test_ensemble)\n",
    "\n",
    "states_test = []\n",
    "total_acc_test = np.zeros(len(current_test))\n",
    "\n",
    "print(\"Running MCMC for test set with ensemble predictions...\")\n",
    "t = time.time()\n",
    "\n",
    "for i in range(Nstep_test):\n",
    "    proposal = current_test + np.random.randn(*current_test.shape) * sigma_test\n",
    "    proposal_logprob = logp_posterior_ensemble(proposal, y_pred_test_ensemble)\n",
    "    \n",
    "    acc_logprob = proposal_logprob - curr_logprob_test\n",
    "    acc_logprob[acc_logprob > 0] = 0\n",
    "    \n",
    "    acc_prob = np.exp(acc_logprob)\n",
    "    acc = np.random.uniform(size=len(current_test)) < acc_prob\n",
    "    \n",
    "    total_acc_test += acc_prob\n",
    "    current_test[acc] = proposal[acc]\n",
    "    curr_logprob_test[acc] = proposal_logprob[acc]\n",
    "    \n",
    "    states_test.append(np.copy(current_test)[None])\n",
    "    \n",
    "    if i % (0.1*Nstep_test) == 0.1*Nstep_test-1:\n",
    "        print(\n",
    "            'Step:', len(states_test),\n",
    "            'Time:', f'{time.time() - t:.2f}s',\n",
    "            'Min acceptance rate:', f'{np.min(total_acc_test / (i + 1)):.4f}',\n",
    "            'Mean acceptance rate:', f'{np.mean(total_acc_test / (i + 1)):.4f}'\n",
    "        )\n",
    "        t = time.time()\n",
    "\n",
    "# Remove burn-in\n",
    "states_test = np.concatenate(states_test[int(0.2*Nstep_test):], 0)\n",
    "\n",
    "# Mean and std of samples\n",
    "mean_test_ensemble = np.mean(states_test, 0)\n",
    "errorbar_test_ensemble = np.std(states_test, 0)\n",
    "\n",
    "print(f\"\\nMCMC complete. Mean error bars: {np.mean(errorbar_test_ensemble, 0)}\")\n",
    "print(f\"Sample means (first 5):\\n{mean_test_ensemble[:5]}\")\n",
    "print(f\"Sample error bars (first 5):\\n{errorbar_test_ensemble[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission file for CNN-RF Ensemble\n",
    "\n",
    "data_ensemble = {\n",
    "    \"means\": mean_test_ensemble.tolist(),\n",
    "    \"errorbars\": errorbar_test_ensemble.tolist()\n",
    "}\n",
    "\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "zip_file_name_ensemble = 'Submission_Ensemble_' + the_date + '.zip'\n",
    "\n",
    "zip_file_ensemble = Utility.save_json_zip(\n",
    "    submission_dir=\"submissions\",\n",
    "    json_file_name=\"result.json\",\n",
    "    zip_file_name=zip_file_name_ensemble,\n",
    "    data=data_ensemble\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUBMISSION FILE CREATED (CNN-RF Ensemble)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Submission ZIP saved at: {zip_file_ensemble}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f28eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: CNN-RF Ensemble Pipeline\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CNN-RANDOM FOREST ENSEMBLE PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"  1. CNN Feature Extractor:\")\n",
    "print(\"     - 4 convolutional blocks with batch normalization\")\n",
    "print(\"     - Extracts 128-dimensional features from convergence maps\")\n",
    "print(\"     - Trained with auxiliary prediction head using MSE loss\")\n",
    "print(\"\\n  2. Random Forest Regressor:\")\n",
    "print(f\"     - n_estimators: {ensemble_config.RF_PARAMS['n_estimators']}\")\n",
    "print(f\"     - max_depth: {ensemble_config.RF_PARAMS['max_depth']}\")\n",
    "print(f\"     - Trained on CNN-extracted features\")\n",
    "print(\"\\n  3. MCMC Uncertainty Quantification:\")\n",
    "print(\"     - Metropolis-Hastings sampling\")\n",
    "print(\"     - 10,000 steps with 20% burn-in\")\n",
    "print(\"\\nAdvantages of CNN-RF Ensemble:\")\n",
    "print(\"  ✓ CNN captures spatial features and patterns\")\n",
    "print(\"  ✓ Random Forest provides non-linear combination of features\")\n",
    "print(\"  ✓ Ensemble reduces overfitting compared to pure neural network\")\n",
    "print(\"  ✓ Random Forest is less prone to gradient issues\")\n",
    "print(\"  ✓ Better generalization on unseen data\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1a905-64ac-4d9e-ae98-ca37efe709bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN architecture for parameter estimation\n",
    "\n",
    "class Simple_CNN(nn.Module):\n",
    "    def __init__(self, height, width, num_targets, dropout1=0.2, dropout2=0.1):\n",
    "        super(Simple_CNN, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self._feature_size = self._get_conv_output_size(height, width)\n",
    "        \n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout1),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout2),\n",
    "            nn.Linear(128, num_targets)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_size(self, height, width):\n",
    "        dummy_input = torch.zeros(1, 1, height, width)\n",
    "        output = self.conv_stack(dummy_input)\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.fc_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Feature Extractor - modified to output features instead of predictions\n",
    "\n",
    "class CNN_FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN that extracts features from the penultimate layer\n",
    "    instead of making direct predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, height, width, feature_dim=128, dropout1=0.2, dropout2=0.1):\n",
    "        super(CNN_FeatureExtractor, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self._feature_size = self._get_conv_output_size(height, width)\n",
    "        \n",
    "        # Feature extraction layers (no final prediction layer)\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout1),\n",
    "            nn.Linear(512, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout2)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_size(self, height, width):\n",
    "        dummy_input = torch.zeros(1, 1, height, width)\n",
    "        output = self.conv_stack(dummy_input)\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.feature_extractor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# CNN-Random Forest Ensemble\n",
    "class CNN_RF_Ensemble:\n",
    "    \"\"\"\n",
    "    Ensemble model combining CNN feature extraction with Random Forest regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_model, rf_params=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        cnn_model : CNN_FeatureExtractor\n",
    "            Trained CNN feature extractor\n",
    "        rf_params : dict, optional\n",
    "            Parameters for RandomForestRegressor\n",
    "        \"\"\"\n",
    "        self.cnn_model = cnn_model\n",
    "        self.device = next(cnn_model.parameters()).device\n",
    "        \n",
    "        if rf_params is None:\n",
    "            rf_params = {\n",
    "                'n_estimators': 200,\n",
    "                'max_depth': 20,\n",
    "                'min_samples_split': 5,\n",
    "                'min_samples_leaf': 2,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "        self.rf_model = RandomForestRegressor(**rf_params)\n",
    "        \n",
    "    def extract_features(self, dataloader):\n",
    "        \"\"\"Extract features from images using CNN\"\"\"\n",
    "        self.cnn_model.eval()\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "                if len(batch) == 2:\n",
    "                    X, y = batch\n",
    "                    X = X.to(self.device)\n",
    "                    features = self.cnn_model(X)\n",
    "                    features_list.append(features.cpu().numpy())\n",
    "                    labels_list.append(y.numpy())\n",
    "                else:\n",
    "                    X = batch\n",
    "                    X = X.to(self.device)\n",
    "                    features = self.cnn_model(X)\n",
    "                    features_list.append(features.cpu().numpy())\n",
    "        \n",
    "        features = np.concatenate(features_list, axis=0)\n",
    "        if labels_list:\n",
    "            labels = np.concatenate(labels_list, axis=0)\n",
    "            return features, labels\n",
    "        return features\n",
    "    \n",
    "    def fit_rf(self, train_loader):\n",
    "        \"\"\"Train Random Forest on CNN features\"\"\"\n",
    "        print(\"Extracting training features...\")\n",
    "        X_train_features, y_train = self.extract_features(train_loader)\n",
    "        \n",
    "        print(f\"Training Random Forest on {X_train_features.shape[0]} samples...\")\n",
    "        self.rf_model.fit(X_train_features, y_train)\n",
    "        print(\"Random Forest training complete!\")\n",
    "        \n",
    "    def predict(self, test_loader):\n",
    "        \"\"\"Make predictions using CNN features + Random Forest\"\"\"\n",
    "        print(\"Extracting test features...\")\n",
    "        X_test_features = self.extract_features(test_loader)\n",
    "        \n",
    "        print(\"Making predictions with Random Forest...\")\n",
    "        predictions = self.rf_model.predict(X_test_features)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43fb9d",
   "metadata": {},
   "source": [
    "# CNN-Random Forest Ensemble Approach\n",
    "\n",
    "## Overview\n",
    "This section implements an **ensemble learning approach** that combines the strengths of Convolutional Neural Networks (CNNs) and Random Forests:\n",
    "\n",
    "### Architecture\n",
    "1. **CNN Feature Extractor**: \n",
    "   - Processes convergence maps through convolutional layers\n",
    "   - Extracts high-level spatial features (128-dimensional vectors)\n",
    "   - Trained with an auxiliary prediction head using supervised learning\n",
    "\n",
    "2. **Random Forest Regressor**:\n",
    "   - Takes CNN-extracted features as input\n",
    "   - Learns non-linear relationships between features and cosmological parameters\n",
    "   - Provides robust predictions with built-in feature importance\n",
    "\n",
    "3. **MCMC Uncertainty Quantification**:\n",
    "   - Uses Metropolis-Hastings sampling to estimate posterior distributions\n",
    "   - Provides confidence intervals for predictions\n",
    "\n",
    "### Advantages\n",
    "- **Better Feature Learning**: CNN learns spatial patterns in convergence maps\n",
    "- **Improved Generalization**: Random Forest reduces overfitting\n",
    "- **Robustness**: Ensemble approach is more stable than single model\n",
    "- **Interpretability**: Random Forest provides feature importance insights\n",
    "\n",
    "### Workflow\n",
    "1. Train CNN feature extractor with auxiliary head\n",
    "2. Extract features from training data\n",
    "3. Train Random Forest on extracted features\n",
    "4. Make predictions by passing data through CNN → RF pipeline\n",
    "5. Use MCMC for uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd539c2e-44d8-4e04-b366-eb8f93df596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Training\")\n",
    "    for X, y in pbar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Validating\")\n",
    "    with torch.no_grad():\n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b477fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for CNN Feature Extractor (with auxiliary prediction head for training)\n",
    "\n",
    "class CNN_FeatureExtractor_WithHead(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Feature Extractor with an auxiliary prediction head for training.\n",
    "    The prediction head is used during training but discarded during feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, height, width, num_targets, feature_dim=128, dropout1=0.2, dropout2=0.1):\n",
    "        super(CNN_FeatureExtractor_WithHead, self).__init__()\n",
    "        self.feature_extractor = CNN_FeatureExtractor(height, width, feature_dim, dropout1, dropout2)\n",
    "        self.prediction_head = nn.Linear(feature_dim, num_targets)\n",
    "        \n",
    "    def forward(self, x, return_features=False):\n",
    "        features = self.feature_extractor(x)\n",
    "        if return_features:\n",
    "            return features\n",
    "        predictions = self.prediction_head(features)\n",
    "        return predictions\n",
    "    \n",
    "    def get_feature_extractor(self):\n",
    "        \"\"\"Return just the feature extractor without the prediction head\"\"\"\n",
    "        return self.feature_extractor\n",
    "\n",
    "\n",
    "def train_epoch_ensemble(model, dataloader, loss_fn, optimizer, device):\n",
    "    \"\"\"Training epoch for CNN with auxiliary head\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Training CNN Feature Extractor\")\n",
    "    for X, y in pbar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(X, return_features=False)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch_ensemble(model, dataloader, loss_fn, device):\n",
    "    \"\"\"Validation epoch for CNN with auxiliary head\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Validating CNN Feature Extractor\")\n",
    "    with torch.no_grad():\n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X, return_features=False)\n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "            \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ba1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    dropout1 = trial.suggest_float('dropout1', 0.0, 0.5)\n",
    "    dropout2 = trial.suggest_float('dropout2', 0.0, 0.5)\n",
    "    epochs = trial.suggest_int('epochs', 3, 10)\n",
    "    \n",
    "    # Create model\n",
    "    model = Simple_CNN(config.IMG_HEIGHT, config.IMG_WIDTH, config.NUM_TARGETS, dropout1, dropout2).to(config.DEVICE)\n",
    "    \n",
    "    # Create dataloaders with suggested batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Train for suggested epochs\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, config.DEVICE)\n",
    "        val_loss = validate_epoch(model, val_loader, loss_fn, config.DEVICE)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna hyperparameter optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0171198-8a56-47a4-9d1c-bf754257d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosmologyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels=None,\n",
    "                 transform=None,\n",
    "                 label_transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx].astype(np.float32)   # Convert from float16 to float32\n",
    "        if self.transform:\n",
    "            image = self.transform(image) \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx].astype(np.float32)\n",
    "            label = torch.from_numpy(label)\n",
    "            if self.label_transform:\n",
    "                label = self.label_transform(label)\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77aca1f-202f-4213-a3ea-e2636120d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the means and stds of the training images (for standardizing the data)\n",
    "\n",
    "means = np.mean(X_train, dtype=np.float32)\n",
    "stds = np.std(X_train, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80b5a6-9c1b-4be4-abed-0b061055d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image standardization\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),     \n",
    "    transforms.Normalize(mean=[means], std=[stds]),   \n",
    "])\n",
    "print(f\"Image stats (from train set): Mean={means}, Std={stds}\")\n",
    "\n",
    "# Label standardization\n",
    "label_scaler = StandardScaler()\n",
    "y_train_scaled = label_scaler.fit_transform(y_train)\n",
    "y_val_scaled = label_scaler.transform(y_val)\n",
    "print(f\"Label stats (from train set): Mean={label_scaler.mean_}, Std={np.sqrt(label_scaler.var_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f83eef-6a50-4560-a5c0-1036bce7a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration\n",
    "config = Config()\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = CosmologyDataset(\n",
    "    data=X_train, \n",
    "    labels=y_train_scaled,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset = CosmologyDataset(\n",
    "    data=X_val, \n",
    "    labels=y_val_scaled,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d1060-d2fc-4579-87ea-e81eb21926f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CNN model\n",
    "model = Simple_CNN(config.IMG_HEIGHT,\n",
    "                    config.IMG_WIDTH,\n",
    "                    config.NUM_TARGETS,\n",
    "                    config.DROPOUT1,\n",
    "                    config.DROPOUT2).to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e7b94-3579-49ef-b13a-fc60bbb079ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL = False\n",
    "# USE_PRETRAINED_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config with best hyperparameters\n",
    "config.BATCH_SIZE = best_params['batch_size']\n",
    "config.LEARNING_RATE = best_params['learning_rate']\n",
    "config.WEIGHT_DECAY = best_params['weight_decay']\n",
    "config.DROPOUT1 = best_params['dropout1']\n",
    "config.DROPOUT2 = best_params['dropout2']\n",
    "config.EPOCHS = best_params['epochs']\n",
    "\n",
    "# Recreate model with best dropouts\n",
    "model = Simple_CNN(config.IMG_HEIGHT, config.IMG_WIDTH, config.NUM_TARGETS, config.DROPOUT1, config.DROPOUT2).to(config.DEVICE)\n",
    "\n",
    "# Recreate dataloaders with best batch_size\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b93c37-9e07-4260-a1da-938a63ecca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_PRETRAINED_MODEL:  \n",
    "    # Train the model\n",
    "    loss_fn = nn.MSELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=config.LEARNING_RATE,\n",
    "                                weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(optimizer,\n",
    "                                  mode='min',\n",
    "                                  factor=0.5,\n",
    "                                  patience=5)\n",
    "    # Training Loop\n",
    "    best_val_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, config.DEVICE)\n",
    "        val_loss = validate_epoch(model, val_loader, loss_fn, config.DEVICE)\n",
    "    \n",
    "        scheduler.step(val_loss)    \n",
    "        print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Save the best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), config.MODEL_SAVE_PATH)\n",
    "            print(f\"  -> New best model saved to {config.MODEL_SAVE_PATH}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(config.MODEL_SAVE_PATH, weights_only=True)) # Directly load the best model\n",
    "\n",
    "else:\n",
    "    # Check if the pretrained model exists\n",
    "    if os.path.exists(config.MODEL_SAVE_PATH):\n",
    "        # If the pretrained model exists, load the model\n",
    "        model.load_state_dict(torch.load(config.MODEL_SAVE_PATH, weights_only=True))\n",
    "\n",
    "    else:\n",
    "        # If the pretrained model doesn't exist, show the warning message\n",
    "        warning_msg = f\"The path of pretrained model doesn't exist\"\n",
    "        warnings.warn(warning_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b9cb2-1d21-40b7-be72-6a3f4bb3e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []   \n",
    "pbar = tqdm(val_loader, total=len(val_loader), desc=\"Validating\")\n",
    "with torch.no_grad():\n",
    "    for X, _ in pbar:\n",
    "        X = X.to(config.DEVICE)\n",
    "        y_pred = model(X)        \n",
    "        y_pred = label_scaler.inverse_transform(y_pred.cpu().numpy())\n",
    "        y_pred_list.append(y_pred) \n",
    "\n",
    "y_pred_val = np.concatenate(y_pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e771d5e-7570-4f34-8304-c22dc042154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the CNN predictions and the validation labels\n",
    "\n",
    "plt.scatter(y_val[:,0], y_pred_val[:,0])\n",
    "plt.plot(sorted(y_val[:,0]), sorted(y_val[:,0]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,0]), np.max(y_val[:,0]))\n",
    "plt.ylim(0, 0.7)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$\\Omega_m$')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_val[:,1], y_pred_val[:,1])\n",
    "plt.plot(sorted(y_val[:,1]), sorted(y_val[:,1]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,1]), np.max(y_val[:,1]))\n",
    "plt.ylim(0.65, 1)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$S_8$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f399a9-629d-4dd4-b528-1d17517cbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are Ncosmo distinct cosmologies in the labels.\n",
    "# Here we create a list that groups the indices of the validation instances with the same cosmological parameters\n",
    "\n",
    "cosmology = data_obj.label[:,0,:2]   # shape = (Ncosmo, 2)\n",
    "\n",
    "row_to_i = {tuple(cosmology[i]): i for i in range(Ncosmo)}\n",
    "index_lists = [[] for _ in range(cosmology.shape[0])]\n",
    "\n",
    "# Loop over each row in 'y_val' with shape = (Nval, 2)\n",
    "for idx in range(len(y_val)):\n",
    "    row_tuple = tuple(y_val[idx])\n",
    "    i = row_to_i[row_tuple]\n",
    "    index_lists[i].append(idx)\n",
    "\n",
    "# val_cosmology_idx[i] = the indices idx of the validation examples with labels = cosmology[i]\n",
    "val_cosmology_idx = [np.array(lst) for lst in index_lists]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26a7bf-71e8-4e23-82ea-89a82d7f513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summary statistics of all realizations for all cosmologies in the validation set\n",
    "d_vector = []  \n",
    "n_d = 2   # Number of summary statistics for each map\n",
    "for i in range(Ncosmo):\n",
    "    d_i =  np.zeros((len(val_cosmology_idx[i]), n_d))  \n",
    "    for j, idx in enumerate(val_cosmology_idx[i]):\n",
    "        d_i[j] = y_pred_val[idx]\n",
    "\n",
    "    d_vector.append(d_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae67b2-2443-4bc1-b89f-d2821bafc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean summary statistics (average over all realizations)\n",
    "mean_d_vector = []\n",
    "for i in range(Ncosmo):\n",
    "    mean_d_vector.append(np.mean(d_vector[i], 0))\n",
    "mean_d_vector = np.array(mean_d_vector)   \n",
    "\n",
    "# covariance matrix\n",
    "delta = []\n",
    "for i in range(Ncosmo):\n",
    "    delta.append((d_vector[i] - mean_d_vector[i].reshape(1, n_d))) \n",
    "\n",
    "cov_d_vector = [(delta[i].T @ delta[i] / (len(delta[i])-n_d-2))[None] for i in range(Ncosmo)]     \n",
    "cov_d_vector = np.concatenate(cov_d_vector, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005939ee-2be6-4c34-a13f-0c90fb2e1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator\n",
    "mean_d_vector_interp = LinearNDInterpolator(cosmology, mean_d_vector, fill_value=np.nan)\n",
    "cov_d_vector_interp = LinearNDInterpolator(cosmology, cov_d_vector, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf041cb-e52c-46a9-a657-b8047363f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior_interp = LinearNDInterpolator(cosmology, np.zeros((Ncosmo, 1)), fill_value=-np.inf)\n",
    "\n",
    "# Note that the training data are not uniformly sampled, which introduces a prior distribution. Here we ignore that prior for simplicity.\n",
    "# Also note that this prior would introduce bias for cosmologies at the boundary of the prior\n",
    "def log_prior(x):\n",
    "    logprior = logprior_interp(x).flatten()  # shape = (Ntest, ) \n",
    "    return logprior\n",
    "\n",
    "# Gaussian likelihood with interpolated mean and covariance matrix\n",
    "def loglike(x, d):\n",
    "    mean = mean_d_vector_interp(x) \n",
    "    cov = cov_d_vector_interp(x)   \n",
    "    delta = d - mean               \n",
    "    \n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    cov_det = np.linalg.slogdet(cov)[1]\n",
    "    \n",
    "    return -0.5 * cov_det - 0.5 * np.einsum(\"ni,nij,nj->n\", delta, inv_cov, delta)\n",
    "\n",
    "def logp_posterior(x, d):\n",
    "    logp = log_prior(x)\n",
    "    select = np.isfinite(logp)\n",
    "    if np.sum(select) > 0:\n",
    "        logp[select] = logp[select] + loglike(x[select], d[select])\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c282a-bbce-4ddd-8ace-4ce7129e3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sampling to explore the posterior distribution\n",
    "\n",
    "Nstep = 10000  # Number of MCMC steps (iterations)\n",
    "sigma = 0.06   # Proposal standard deviation; should be tuned per method or parameter scale\n",
    "\n",
    "# Randomly select initial points from the `cosmology` array for each test case\n",
    "# Assumes `cosmology` has shape (Ncosmo, ndim) and `Ntest` is the number of independent chains/samples\n",
    "current = cosmology[np.random.choice(Ncosmo, size=Nval)]\n",
    "\n",
    "# Compute log-posterior at the initial points\n",
    "curr_logprob = logp_posterior(current, y_pred_val)\n",
    "\n",
    "# List to store sampled states (for all chains)\n",
    "states = []\n",
    "\n",
    "# Track total acceptance probabilities to compute acceptance rates\n",
    "total_acc = np.zeros(len(current))\n",
    "\n",
    "t = time.time()  # Track time for performance reporting\n",
    "\n",
    "# MCMC loop\n",
    "for i in range(Nstep):\n",
    "\n",
    "    # Generate proposals by adding Gaussian noise to current state\n",
    "    proposal = current + np.random.randn(*current.shape) * sigma    \n",
    "\n",
    "    # Compute log-posterior at the proposed points\n",
    "    proposal_logprob = logp_posterior(proposal, y_pred_val)\n",
    "\n",
    "    # Compute log acceptance ratio (Metropolis-Hastings)\n",
    "    acc_logprob = proposal_logprob - curr_logprob\n",
    "    acc_logprob[acc_logprob > 0] = 0  # Cap at 0 to avoid exp overflow (acceptance prob ≤ 1)\n",
    "\n",
    "    # Convert to acceptance probabilities\n",
    "    acc_prob = np.exp(acc_logprob)\n",
    "\n",
    "    # Decide whether to accept each proposal\n",
    "    acc = np.random.uniform(size=len(current)) < acc_prob\n",
    "\n",
    "    # Track acceptance probabilities (not binary outcomes)\n",
    "    total_acc += acc_prob\n",
    "\n",
    "    # Update states and log-probs where proposals are accepted\n",
    "    current[acc] = proposal[acc]\n",
    "    curr_logprob[acc] = proposal_logprob[acc]\n",
    "\n",
    "    # Save a copy of the current state\n",
    "    states.append(np.copy(current)[None])\n",
    "\n",
    "    # Periodically print progress and acceptance rates\n",
    "    if i % (0.1*Nstep) == 0.1*Nstep-1:\n",
    "        print(\n",
    "            'step:', len(states),\n",
    "            'Time:', time.time() - t,\n",
    "            'Min acceptance rate:', np.min(total_acc / (i + 1)),\n",
    "            'Mean acceptance rate:', np.mean(total_acc / (i + 1))\n",
    "        )\n",
    "        t = time.time()  # Reset timer for next print interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d221020-1f20-452c-8f30-8384643a5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove burn-in\n",
    "states = np.concatenate(states[int(0.2*Nstep):], 0)\n",
    "\n",
    "# mean and std of samples\n",
    "mean_val = np.mean(states, 0)\n",
    "errorbar_val = np.std(states, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c417b0-01f4-4dc4-b301-d609edaec917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the means & standard deviations of the posterior distributions and the validation labels\n",
    "\n",
    "plt.errorbar(y_val[:,0], mean_val[:,0], yerr=errorbar_val[:,0], \n",
    "             fmt='o', capsize=3, capthick=1, ecolor='grey')\n",
    "plt.plot(sorted(y_val[:,0]), sorted(y_val[:,0]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,0]), np.max(y_val[:,0]))\n",
    "plt.ylim(0, 0.7)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$\\Omega_m$')\n",
    "plt.show()\n",
    "\n",
    "plt.errorbar(y_val[:,1], mean_val[:,1], yerr=errorbar_val[:,1], \n",
    "             fmt='o', capsize=3, capthick=1, ecolor='grey')\n",
    "plt.plot(sorted(y_val[:,1]), sorted(y_val[:,1]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,1]), np.max(y_val[:,1]))\n",
    "plt.ylim(0.65, 1)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$S_8$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff7f13-5d73-4df3-badf-cf72bdc946b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_score = Score._score_phase1(\n",
    "    true_cosmo=y_val,\n",
    "    infer_cosmo=mean_val,\n",
    "    errorbar=errorbar_val\n",
    ")\n",
    "print('averaged score:', np.mean(validation_score))\n",
    "print('averaged error bar:', np.mean(errorbar_val, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2f916-2aba-408e-9cf0-8144eb66decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CosmologyDataset(\n",
    "    data=data_obj.kappa_test, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c4191-a14f-4e83-a8ba-1b7c7d641121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []   \n",
    "pbar = tqdm(test_loader, total=len(test_loader), desc=\"Inference on the test set\")\n",
    "with torch.no_grad():\n",
    "    for X in pbar:\n",
    "        X = X.to(config.DEVICE)\n",
    "        y_pred = model(X)        \n",
    "        y_pred = label_scaler.inverse_transform(y_pred.cpu().numpy())\n",
    "        y_pred_list.append(y_pred) \n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edafeab-6d65-4fc6-9b2f-d596774f2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sampling to explore the posterior distribution\n",
    "\n",
    "Nstep = 10000  # Number of MCMC steps (iterations)\n",
    "sigma = 0.06   # Proposal standard deviation; should be tuned per method or parameter scale\n",
    "\n",
    "# Randomly select initial points from the `cosmology` array for each test case\n",
    "# Assumes `cosmology` has shape (Ncosmo, ndim) and `Ntest` is the number of independent chains/samples\n",
    "current = cosmology[np.random.choice(Ncosmo, size=data_obj.Ntest)]\n",
    "\n",
    "# Compute log-posterior at the initial points\n",
    "curr_logprob = logp_posterior(current, y_pred_test)\n",
    "\n",
    "# List to store sampled states (for all chains)\n",
    "states = []\n",
    "\n",
    "# Track total acceptance probabilities to compute acceptance rates\n",
    "total_acc = np.zeros(len(current))\n",
    "\n",
    "t = time.time()  # Track time for performance reporting\n",
    "\n",
    "# MCMC loop\n",
    "for i in range(Nstep):\n",
    "\n",
    "    # Generate proposals by adding Gaussian noise to current state\n",
    "    proposal = current + np.random.randn(*current.shape) * sigma    \n",
    "\n",
    "    # Compute log-posterior at the proposed points\n",
    "    proposal_logprob = logp_posterior(proposal, y_pred_test)\n",
    "\n",
    "    # Compute log acceptance ratio (Metropolis-Hastings)\n",
    "    acc_logprob = proposal_logprob - curr_logprob\n",
    "    acc_logprob[acc_logprob > 0] = 0  # Cap at 0 to avoid exp overflow (acceptance prob ≤ 1)\n",
    "\n",
    "    # Convert to acceptance probabilities\n",
    "    acc_prob = np.exp(acc_logprob)\n",
    "\n",
    "    # Decide whether to accept each proposal\n",
    "    acc = np.random.uniform(size=len(current)) < acc_prob\n",
    "\n",
    "    # Track acceptance probabilities (not binary outcomes)\n",
    "    total_acc += acc_prob\n",
    "\n",
    "    # Update states and log-probs where proposals are accepted\n",
    "    current[acc] = proposal[acc]\n",
    "    curr_logprob[acc] = proposal_logprob[acc]\n",
    "\n",
    "    # Save a copy of the current state\n",
    "    states.append(np.copy(current)[None])\n",
    "\n",
    "    # Periodically print progress and acceptance rates\n",
    "    if i % (0.1*Nstep) == 0.1*Nstep-1:\n",
    "        print(\n",
    "            'step:', len(states),\n",
    "            'Time:', time.time() - t,\n",
    "            'Min acceptance rate:', np.min(total_acc / (i + 1)),\n",
    "            'Mean acceptance rate:', np.mean(total_acc / (i + 1))\n",
    "        )\n",
    "        t = time.time()  # Reset timer for next print interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e4ca8-a9f6-4753-9ca0-6d7c3e8eedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove burn-in\n",
    "states = np.concatenate(states[int(0.2*Nstep):], 0)\n",
    "\n",
    "# mean and std of samples\n",
    "mean = np.mean(states, 0)\n",
    "errorbar = np.std(states, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2700348",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"means\": mean.tolist(), \"errorbars\": errorbar.tolist()}\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "zip_file_name = 'Submission_' + the_date + '.zip'\n",
    "zip_file = Utility.save_json_zip(\n",
    "    submission_dir=\"submissions\",\n",
    "    json_file_name=\"result.json\",\n",
    "    zip_file_name=zip_file_name,\n",
    "    data=data\n",
    ")\n",
    "print(f\"Submission ZIP saved at: {zip_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
